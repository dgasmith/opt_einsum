{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Optimized einsum can significantly reduce the overall execution time of einsum-like expressions by optimizing the expression's contraction order and dispatching many operations to canonical BLAS, cuBLAS, or other specialized routines. Optimized einsum is agnostic to the backend and can handle NumPy, Dask, PyTorch, Tensorflow, CuPy, Sparse, Theano, JAX, and Autograd arrays as well as potentially any library which conforms to a standard API.</p>"},{"location":"#features","title":"Features","text":"<p>The algorithms found in this repository often power the <code>einsum</code> optimizations in many of the above projects. For example, the optimization of <code>np.einsum</code> has been passed upstream and most of the same features that can be found in this repository can be enabled with <code>numpy.einsum(..., optimize=True)</code>. However, this repository often has more up to date algorithms for complex contractions. Several advanced features are as follows:</p> <ul> <li>Inspect detailed information about the path chosen.</li> <li>Perform contractions with numerous backends, including on the GPU and with libraries such as TensorFlow and PyTorch.</li> <li>Generate reusable expressions, potentially with constant tensors, that can be compiled for greater performance.</li> <li>Use an arbitrary number of indices to find contractions for hundreds or even thousands of tensors.</li> <li>Share intermediate computations among multiple contractions.</li> <li>Compute gradients of tensor contractions using Autograd or JAX.</li> </ul>"},{"location":"#example","title":"Example","text":"<p>Take the following einsum-like expression:</p> \\[ M_{pqrs} = C_{pi} C_{qj} I_{ijkl} C_{rk} C_{sl} \\] <p>and consider two different algorithms:</p> <pre><code>import numpy as np\n\ndim = 10\nI = np.random.rand(dim, dim, dim, dim)\nC = np.random.rand(dim, dim)\n\ndef naive(I, C):\n    # N^8 scaling\n    return np.einsum('pi,qj,ijkl,rk,sl-&gt;pqrs', C, C, I, C, C)\n\ndef optimized(I, C):\n    # N^5 scaling\n    K = np.einsum('pi,ijkl-&gt;pjkl', C, I)\n    K = np.einsum('qj,pjkl-&gt;pqkl', C, K)\n    K = np.einsum('rk,pqkl-&gt;pqrl', C, K)\n    K = np.einsum('sl,pqrl-&gt;pqrs', C, K)\n    return K\n</code></pre> <pre><code>&gt;&gt;&gt; np.allclose(naive(I, C), optimized(I, C))\nTrue\n</code></pre> <p>Most einsum functions do not consider building intermediate arrays; therefore, helping einsum functions by creating these intermediate arrays can result in considerable cost savings even for small N (N=10):</p> <pre><code>%timeit naive(I, C)\n1 loops, best of 3: 829 ms per loop\n\n%timeit optimized(I, C)\n1000 loops, best of 3: 445 \u00b5s per loop\n</code></pre> <p>The index transformation is a well-known contraction that leads to straightforward intermediates. This contraction can be further complicated by considering that the shape of the C matrices need not be the same, in this case, the ordering in which the indices are transformed matters significantly. Logic can be built that optimizes the order; however, this is a lot of time and effort for a single expression.</p> <p>The <code>opt_einsum</code> package is a typically a drop-in replacement for <code>einsum</code> functions and can handle this logic and path finding for you:</p> <pre><code>from opt_einsum import contract\n\ndim = 30\nI = np.random.rand(dim, dim, dim, dim)\nC = np.random.rand(dim, dim)\n\n%timeit optimized(I, C)\n10 loops, best of 3: 65.8 ms per loop\n\n%timeit contract('pi,qj,ijkl,rk,sl-&gt;pqrs', C, C, I, C, C)\n100 loops, best of 3: 16.2 ms per loop\n</code></pre> <p>The above will automatically find the optimal contraction order, in this case, identical to that of the optimized function above, and compute the products for you. Additionally, <code>contract</code> can use vendor BLAS with the <code>numpy.dot</code> function under the hood to exploit additional parallelism and performance.</p> <p>Details about the optimized contraction order can be explored:</p> <pre><code>&gt;&gt;&gt; import opt_einsum as oe\n\n&gt;&gt;&gt; path_info = oe.contract_path('pi,qj,ijkl,rk,sl-&gt;pqrs', C, C, I, C, C)\n\n&gt;&gt;&gt; print(path_info[0])\n[(0, 2), (0, 3), (0, 2), (0, 1)]\n\n&gt;&gt;&gt; print(path_info[1])\n  Complete contraction:  pi,qj,ijkl,rk,sl-&gt;pqrs\n         Naive scaling:  8\n     Optimized scaling:  5\n      Naive FLOP count:  8.000e+08\n  Optimized FLOP count:  8.000e+05\n   Theoretical speedup:  1000.000\n  Largest intermediate:  1.000e+04 elements\n--------------------------------------------------------------------------------\nscaling   BLAS                  current                                remaining\n--------------------------------------------------------------------------------\n   5      GEMM            ijkl,pi-&gt;jklp                      qj,rk,sl,jklp-&gt;pqrs\n   5      GEMM            jklp,qj-&gt;klpq                         rk,sl,klpq-&gt;pqrs\n   5      GEMM            klpq,rk-&gt;lpqr                            sl,lpqr-&gt;pqrs\n   5      GEMM            lpqr,sl-&gt;pqrs                               pqrs-&gt;pqrs\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If this code has benefited your research, please support us by citing:</p> <p>Daniel G. A. Smith and Johnnie Gray, opt_einsum - A Python package for optimizing contraction order for einsum-like expressions. Journal of Open Source Software, 2018, 3(26), 753</p> <p>DOI: https://doi.org/10.21105/joss.00753</p>"},{"location":"api_reference/","title":"API Documentation","text":""},{"location":"api_reference/#opt_einsumcontract","title":"<code>opt_einsum.contract</code>","text":""},{"location":"api_reference/#opt_einsum.contract.contract","title":"contract","text":"<pre><code>contract(\n    subscripts: str,\n    *operands: ArrayType,\n    out: ArrayType = ...,\n    use_blas: bool = ...,\n    optimize: OptimizeKind = ...,\n    memory_limit: _MemoryLimit = ...,\n    backend: BackendType = ...,\n    **kwargs: Any\n) -&gt; ArrayType\n</code></pre><pre><code>contract(\n    subscripts: ArrayType,\n    *operands: Union[ArrayType, Collection[int]],\n    out: ArrayType = ...,\n    use_blas: bool = ...,\n    optimize: OptimizeKind = ...,\n    memory_limit: _MemoryLimit = ...,\n    backend: BackendType = ...,\n    **kwargs: Any\n) -&gt; ArrayType\n</code></pre> <pre><code>contract(\n    subscripts: Union[str, ArrayType],\n    *operands: Union[ArrayType, Collection[int]],\n    out: Optional[ArrayType] = None,\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    backend: BackendType = \"auto\",\n    **kwargs: Any\n) -&gt; ArrayType\n</code></pre> <p>Evaluates the Einstein summation convention on the operands. A drop in replacement for NumPy's einsum function that optimizes the order of contraction to reduce overall scaling at the cost of several intermediate arrays.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>ArrayType</code>           \u2013            <p>The result of the einsum expression.</p> </li> </ul> Notes <p>This function should produce a result identical to that of NumPy's einsum function. The primary difference is <code>contract</code> will attempt to form intermediates which reduce the overall scaling of the given einsum contraction. By default the worst intermediate formed will be equal to that of the largest input array. For large einsum expressions with many input arrays this can provide arbitrarily large (1000 fold+) speed improvements.</p> <p>For contractions with just two tensors this function will attempt to use NumPy's built-in BLAS functionality to ensure that the given operation is performed optimally. When NumPy is linked to a threaded BLAS, potential speedups are on the order of 20-100 for a six core machine.</p>"},{"location":"api_reference/#opt_einsum.contract.contract(subscripts)","title":"<code>subscripts</code>","text":"(<code>Union[str, ArrayType]</code>)           \u2013            <p>Specifies the subscripts for summation.</p>"},{"location":"api_reference/#opt_einsum.contract.contract(*operands)","title":"<code>*operands</code>","text":"(<code>Union[ArrayType, Collection[int]]</code>, default:                   <code>()</code> )           \u2013            <p>These are the arrays for the operation.</p>"},{"location":"api_reference/#opt_einsum.contract.contract(out)","title":"<code>out</code>","text":"(<code>Optional[ArrayType]</code>, default:                   <code>None</code> )           \u2013            <p>A output array in which set the resulting output.</p>"},{"location":"api_reference/#opt_einsum.contract.contract(use_blas)","title":"<code>use_blas</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Do you use BLAS for valid operations, may use extra memory for more intermediates.</p>"},{"location":"api_reference/#opt_einsum.contract.contract(optimize)","title":"<code>optimize</code>","text":"(<code>OptimizeKind</code>, default:                   <code>True</code> )           \u2013            <ul> <li>Choose the type of path the contraction will be optimized with</li> <li>if a list is given uses this as the path.</li> <li><code>'optimal'</code> An algorithm that explores all possible ways of contracting the listed tensors. Scales factorially with the number of terms in the contraction.</li> <li><code>'dp'</code> A faster (but essentially optimal) algorithm that uses dynamic programming to exhaustively search all contraction paths without outer-products.</li> <li><code>'greedy'</code> An cheap algorithm that heuristically chooses the best pairwise contraction at each step. Scales linearly in the number of terms in the contraction.</li> <li><code>'random-greedy'</code> Run a randomized version of the greedy algorithm 32 times and pick the best path.</li> <li><code>'random-greedy-128'</code> Run a randomized version of the greedy algorithm 128 times and pick the best path.</li> <li><code>'branch-all'</code> An algorithm like optimal but that restricts itself to searching 'likely' paths. Still scales factorially.</li> <li><code>'branch-2'</code> An even more restricted version of 'branch-all' that only searches the best two options at each step. Scales exponentially with the number of terms in the contraction.</li> <li><code>'auto', None, True</code> Choose the best of the above algorithms whilst aiming to keep the path finding time below 1ms.</li> <li><code>'auto-hq'</code> Aim for a high quality contraction, choosing the best of the above algorithms whilst aiming to keep the path finding time below 1sec.</li> <li><code>False</code> will not optimize the contraction.</li> </ul>"},{"location":"api_reference/#opt_einsum.contract.contract(memory_limit)","title":"<code>memory_limit</code>","text":"(<code>_MemoryLimit</code>, default:                   <code>None</code> )           \u2013            <ul> <li>Give the upper bound of the largest intermediate tensor contract will build.</li> <li>None or -1 means there is no limit.</li> <li><code>max_input</code> means the limit is set as largest input tensor.</li> <li>A positive integer is taken as an explicit limit on the number of elements.</li> </ul> <p>The default is None. Note that imposing a limit can make contractions exponentially slower to perform.</p>"},{"location":"api_reference/#opt_einsum.contract.contract(backend)","title":"<code>backend</code>","text":"(<code>BackendType</code>, default:                   <code>'auto'</code> )           \u2013            <p>Which library to use to perform the required <code>tensordot</code>, <code>transpose</code> and <code>einsum</code> calls. Should match the types of arrays supplied, See <code>contract_expression</code> for generating expressions which convert numpy arrays to and from the backend library automatically.</p>"},{"location":"api_reference/#opt_einsumcontract_path","title":"<code>opt_einsum.contract_path</code>","text":""},{"location":"api_reference/#opt_einsum.contract.contract_path","title":"contract_path","text":"<pre><code>contract_path(\n    subscripts: str,\n    *operands: ArrayType,\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    shapes: bool = False,\n    **kwargs: Any\n) -&gt; Tuple[PathType, PathInfo]\n</code></pre><pre><code>contract_path(\n    subscripts: ArrayType,\n    *operands: Union[ArrayType, Collection[int]],\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    shapes: bool = False,\n    **kwargs: Any\n) -&gt; Tuple[PathType, PathInfo]\n</code></pre> <pre><code>contract_path(\n    subscripts: Any,\n    *operands: Any,\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    shapes: bool = False,\n    **kwargs: Any\n) -&gt; Tuple[PathType, PathInfo]\n</code></pre> <p>Find a contraction order <code>path</code>, without performing the contraction.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>PathType</code> )          \u2013            <p>The optimized einsum contraciton path</p> </li> <li> <code>PathInfo</code> (              <code>PathInfo</code> )          \u2013            <p>A printable object containing various information about the path found.</p> </li> </ul> Notes <p>The resulting path indicates which terms of the input contraction should be contracted first, the result of this contraction is then appended to the end of the contraction list.</p> <p>Examples:</p> <p>We can begin with a chain dot example. In this case, it is optimal to contract the b and c tensors represented by the first element of the path (1, 2). The resulting tensor is added to the end of the contraction and the remaining contraction, <code>(0, 1)</code>, is then executed.</p> <pre><code>a = np.random.rand(2, 2)\nb = np.random.rand(2, 5)\nc = np.random.rand(5, 2)\npath_info = opt_einsum.contract_path('ij,jk,kl-&gt;il', a, b, c)\nprint(path_info[0])\n#&gt; [(1, 2), (0, 1)]\nprint(path_info[1])\n#&gt;   Complete contraction:  ij,jk,kl-&gt;il\n#&gt;          Naive scaling:  4\n#&gt;      Optimized scaling:  3\n#&gt;       Naive FLOP count:  1.600e+02\n#&gt;   Optimized FLOP count:  5.600e+01\n#&gt;    Theoretical speedup:  2.857\n#&gt;   Largest intermediate:  4.000e+00 elements\n#&gt; -------------------------------------------------------------------------\n#&gt; scaling                  current                                remaining\n#&gt; -------------------------------------------------------------------------\n#&gt;    3                   kl,jk-&gt;jl                                ij,jl-&gt;il\n#&gt;    3                   jl,ij-&gt;il                                   il-&gt;il\n</code></pre> <p>A more complex index transformation example.</p> <pre><code>I = np.random.rand(10, 10, 10, 10)\nC = np.random.rand(10, 10)\npath_info = oe.contract_path('ea,fb,abcd,gc,hd-&gt;efgh', C, C, I, C, C)\n\nprint(path_info[0])\n#&gt; [(0, 2), (0, 3), (0, 2), (0, 1)]\nprint(path_info[1])\n#&gt;   Complete contraction:  ea,fb,abcd,gc,hd-&gt;efgh\n#&gt;          Naive scaling:  8\n#&gt;      Optimized scaling:  5\n#&gt;       Naive FLOP count:  8.000e+08\n#&gt;   Optimized FLOP count:  8.000e+05\n#&gt;    Theoretical speedup:  1000.000\n#&gt;   Largest intermediate:  1.000e+04 elements\n#&gt; --------------------------------------------------------------------------\n#&gt; scaling                  current                                remaining\n#&gt; --------------------------------------------------------------------------\n#&gt;    5               abcd,ea-&gt;bcde                      fb,gc,hd,bcde-&gt;efgh\n#&gt;    5               bcde,fb-&gt;cdef                         gc,hd,cdef-&gt;efgh\n#&gt;    5               cdef,gc-&gt;defg                            hd,defg-&gt;efgh\n#&gt;    5               defg,hd-&gt;efgh                               efgh-&gt;efgh\n</code></pre>"},{"location":"api_reference/#opt_einsum.contract.contract_path(subscripts)","title":"<code>subscripts</code>","text":"(<code>Any</code>)           \u2013            <p>Specifies the subscripts for summation.</p>"},{"location":"api_reference/#opt_einsum.contract.contract_path(*operands)","title":"<code>*operands</code>","text":"(<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>These are the arrays for the operation.</p>"},{"location":"api_reference/#opt_einsum.contract.contract_path(use_blas)","title":"<code>use_blas</code>","text":"(<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Do you use BLAS for valid operations, may use extra memory for more intermediates.</p>"},{"location":"api_reference/#opt_einsum.contract.contract_path(optimize)","title":"<code>optimize</code>","text":"(<code>OptimizeKind</code>, default:                   <code>True</code> )           \u2013            <p>Choose the type of path the contraction will be optimized with. - if a list is given uses this as the path. - <code>'optimal'</code> An algorithm that explores all possible ways of contracting the listed tensors. Scales factorially with the number of terms in the contraction. - <code>'dp'</code> A faster (but essentially optimal) algorithm that uses dynamic programming to exhaustively search all contraction paths without outer-products. - <code>'greedy'</code> An cheap algorithm that heuristically chooses the best pairwise contraction at each step. Scales linearly in the number of terms in the contraction. - <code>'random-greedy'</code> Run a randomized version of the greedy algorithm 32 times and pick the best path. - <code>'random-greedy-128'</code> Run a randomized version of the greedy algorithm 128 times and pick the best path. - <code>'branch-all'</code> An algorithm like optimal but that restricts itself to searching 'likely' paths. Still scales factorially. - <code>'branch-2'</code> An even more restricted version of 'branch-all' that only searches the best two options at each step. Scales exponentially with the number of terms in the contraction. - <code>'auto'</code> Choose the best of the above algorithms whilst aiming to keep the path finding time below 1ms. - <code>'auto-hq'</code> Aim for a high quality contraction, choosing the best of the above algorithms whilst aiming to keep the path finding time below 1sec.</p>"},{"location":"api_reference/#opt_einsum.contract.contract_path(memory_limit)","title":"<code>memory_limit</code>","text":"(<code>_MemoryLimit</code>, default:                   <code>None</code> )           \u2013            <p>Give the upper bound of the largest intermediate tensor contract will build. - None or -1 means there is no limit - <code>max_input</code> means the limit is set as largest input tensor - a positive integer is taken as an explicit limit on the number of elements</p> <p>The default is None. Note that imposing a limit can make contractions exponentially slower to perform.</p>"},{"location":"api_reference/#opt_einsum.contract.contract_path(shapes)","title":"<code>shapes</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether <code>contract_path</code> should assume arrays (the default) or array shapes have been supplied.</p>"},{"location":"api_reference/#opt_einsumcontract_expression","title":"<code>opt_einsum.contract_expression</code>","text":""},{"location":"api_reference/#opt_einsum.contract.contract_expression","title":"contract_expression","text":"<pre><code>contract_expression(\n    subscripts: str,\n    *operands: Union[ArrayType, TensorShapeType],\n    constants: Union[Collection[int], None] = ...,\n    use_blas: bool = ...,\n    optimize: OptimizeKind = ...,\n    memory_limit: _MemoryLimit = ...,\n    **kwargs: Any\n) -&gt; ContractExpression\n</code></pre><pre><code>contract_expression(\n    subscripts: Union[ArrayType, TensorShapeType],\n    *operands: Union[\n        ArrayType, TensorShapeType, Collection[int]\n    ],\n    constants: Union[Collection[int], None] = ...,\n    use_blas: bool = ...,\n    optimize: OptimizeKind = ...,\n    memory_limit: _MemoryLimit = ...,\n    **kwargs: Any\n) -&gt; ContractExpression\n</code></pre> <pre><code>contract_expression(\n    subscripts: Union[str, ArrayType, TensorShapeType],\n    *shapes: Union[\n        ArrayType, TensorShapeType, Collection[int]\n    ],\n    constants: Union[Collection[int], None] = None,\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    **kwargs: Any\n) -&gt; ContractExpression\n</code></pre> <p>Generate a reusable expression for a given contraction with specific shapes, which can, for example, be cached.</p> <p>Parameters:</p> <pre><code>subscripts: Specifies the subscripts for summation.\nshapes: Shapes of the arrays to optimize the contraction for.\nconstants: The indices of any constant arguments in `shapes`, in which case the\n    actual array should be supplied at that position rather than just a\n    shape. If these are specified, then constant parts of the contraction\n    between calls will be reused. Additionally, if a GPU-enabled backend is\n    used for example, then the constant tensors will be kept on the GPU,\n    minimizing transfers.\nkwargs: Passed on to `contract_path` or `einsum`. See `contract`.\n</code></pre> <p>Returns:</p> <ul> <li> <code>ContractExpression</code>           \u2013            <p>Callable with signature <code>expr(*arrays, out=None, backend='numpy')</code> where the array's shapes should match <code>shapes</code>.</p> </li> </ul> Notes <p>The <code>out</code> keyword argument should be supplied to the generated expression rather than this function. The <code>backend</code> keyword argument should also be supplied to the generated expression. If numpy arrays are supplied, if possible they will be converted to and back from the correct backend array type. The generated expression will work with any arrays which have the same rank (number of dimensions) as the original shapes, however, if the actual sizes are different, the expression may no longer be optimal. Constant operations will be computed upon the first call with a particular backend, then subsequently reused.</p> <p>Examples: Basic usage:</p> <pre><code>expr = contract_expression(\"ab,bc-&gt;ac\", (3, 4), (4, 5))\na, b = np.random.rand(3, 4), np.random.rand(4, 5)\nc = expr(a, b)\nnp.allclose(c, a @ b)\n#&gt; True\n</code></pre> <p>Supply <code>a</code> as a constant:</p> <pre><code>expr = contract_expression(\"ab,bc-&gt;ac\", a, (4, 5), constants=[0])\nexpr\n#&gt; &lt;ContractExpression('[ab],bc-&gt;ac', constants=[0])&gt;\n\nc = expr(b)\nnp.allclose(c, a @ b)\n#&gt; True\n</code></pre>"},{"location":"api_reference/#opt_einsumcontractcontractexpression","title":"<code>opt_einsum.contract.ContractExpression</code>","text":""},{"location":"api_reference/#opt_einsum.contract.ContractExpression","title":"ContractExpression","text":"<pre><code>ContractExpression(\n    contraction: str,\n    contraction_list: ContractionListType,\n    constants_dict: Dict[int, ArrayType],\n    **kwargs: Any\n)\n</code></pre> <p>Helper class for storing an explicit <code>contraction_list</code> which can then be repeatedly called solely with the array arguments.</p> <p>Methods:</p> <ul> <li> <code>evaluate_constants</code>             \u2013              <p>Convert any constant operands to the correct backend form, and</p> </li> </ul>"},{"location":"api_reference/#opt_einsum.contract.ContractExpression.evaluate_constants","title":"evaluate_constants","text":"<pre><code>evaluate_constants(backend: Optional[str] = 'auto') -&gt; None\n</code></pre> <p>Convert any constant operands to the correct backend form, and perform as many contractions as possible to create a new list of operands, stored in <code>self._evaluated_constants[backend]</code>. This also makes sure <code>self.contraction_list</code> only contains the remaining, non-const operations.</p>"},{"location":"api_reference/#opt_einsumcontractpathinfo","title":"<code>opt_einsum.contract.PathInfo</code>","text":""},{"location":"api_reference/#opt_einsum.contract.PathInfo","title":"PathInfo","text":"<pre><code>PathInfo(\n    contraction_list: ContractionListType,\n    input_subscripts: str,\n    output_subscript: str,\n    indices: ArrayIndexType,\n    path: PathType,\n    scale_list: Sequence[int],\n    naive_cost: int,\n    opt_cost: int,\n    size_list: Sequence[int],\n    size_dict: Dict[str, int],\n)\n</code></pre> <p>A printable object to contain information about a contraction path.</p>"},{"location":"api_reference/#opt_einsumget_symbol","title":"<code>opt_einsum.get_symbol</code>","text":""},{"location":"api_reference/#opt_einsum.parser.get_symbol","title":"get_symbol","text":"<pre><code>get_symbol(i: int) -&gt; str\n</code></pre> <p>Get the symbol corresponding to int <code>i</code> - runs through the usual 52 letters before resorting to unicode characters, starting at <code>chr(192)</code> and skipping surrogates.</p> <p>Examples:</p> <pre><code>get_symbol(2)\n#&gt; 'c'\n\nget_symbol(200)\n#&gt; '\u0154'\n\nget_symbol(20000)\n#&gt; '\u4eac'\n</code></pre>"},{"location":"api_reference/#opt_einsumshared_intermediates","title":"<code>opt_einsum.shared_intermediates</code>","text":""},{"location":"api_reference/#opt_einsum.sharing.shared_intermediates","title":"shared_intermediates","text":"<pre><code>shared_intermediates(\n    cache: Optional[CacheType] = None,\n) -&gt; Generator[CacheType, None, None]\n</code></pre> <p>Context in which contract intermediate results are shared.</p> <p>Note that intermediate computations will not be garbage collected until 1. this context exits, and 2. the yielded cache is garbage collected (if it was captured).</p> <p>Parameters:</p> <ul> <li>cache - (dict) If specified, a user-stored dict in which intermediate results will be stored. This can be used to interleave sharing contexts.</li> </ul> <p>Returns:</p> <ul> <li>cache - (dict) A dictionary in which sharing results are stored. If ignored,     sharing results will be garbage collected when this context is     exited. This dict can be passed to another context to resume     sharing.</li> </ul>"},{"location":"api_reference/#opt_einsumpathsoptimal","title":"<code>opt_einsum.paths.optimal</code>","text":""},{"location":"api_reference/#opt_einsum.paths.optimal","title":"optimal","text":"<pre><code>optimal(\n    inputs: List[ArrayIndexType],\n    output: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n) -&gt; PathType\n</code></pre> <p>Computes all possible pair contractions in a depth-first recursive manner, sieving results based on <code>memory_limit</code> and the best path found so far.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>PathType</code> )          \u2013            <p>The optimal contraction order within the memory limit constraint.</p> </li> </ul> <p>Examples: <pre><code>isets = [set('abd'), set('ac'), set('bdc')]\noset = set('')\nidx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\noptimal(isets, oset, idx_sizes, 5000)\n#&gt; [(0, 2), (0, 1)]\n</code></pre></p>"},{"location":"api_reference/#opt_einsum.paths.optimal(inputs)","title":"<code>inputs</code>","text":"(<code>List[ArrayIndexType]</code>)           \u2013            <p>List of sets that represent the lhs side of the einsum subscript.</p>"},{"location":"api_reference/#opt_einsum.paths.optimal(output)","title":"<code>output</code>","text":"(<code>ArrayIndexType</code>)           \u2013            <p>Set that represents the rhs side of the overall einsum subscript.</p>"},{"location":"api_reference/#opt_einsum.paths.optimal(size_dict)","title":"<code>size_dict</code>","text":"(<code>Dict[str, int]</code>)           \u2013            <p>Dictionary of index sizes.</p>"},{"location":"api_reference/#opt_einsum.paths.optimal(memory_limit)","title":"<code>memory_limit</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of elements in a temporary array.</p>"},{"location":"api_reference/#opt_einsumpathsgreedy","title":"<code>opt_einsum.paths.greedy</code>","text":""},{"location":"api_reference/#opt_einsum.paths.greedy","title":"greedy","text":"<pre><code>greedy(\n    inputs: List[ArrayIndexType],\n    output: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n    choose_fn: Any = None,\n    cost_fn: str = \"memory-removed\",\n) -&gt; PathType\n</code></pre> <p>Finds the path by a three stage algorithm:</p> <ol> <li>Eagerly compute Hadamard products.</li> <li>Greedily compute contractions to maximize <code>removed_size</code></li> <li>Greedily compute outer products.</li> </ol> <p>This algorithm scales quadratically with respect to the maximum number of elements sharing a common dim.</p> <p>Parameters:</p> <ul> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> <li> </li> </ul> <p>Returns:</p> <ul> <li> <code>path</code> (              <code>PathType</code> )          \u2013            <p>The contraction order (a list of tuples of ints).</p> </li> </ul> <p>Examples:</p> <pre><code>isets = [set('abd'), set('ac'), set('bdc')]\noset = set('')\nidx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\ngreedy(isets, oset, idx_sizes)\n#&gt; [(0, 2), (0, 1)]\n</code></pre>"},{"location":"api_reference/#opt_einsum.paths.greedy(inputs)","title":"<code>inputs</code>","text":"(<code>List[ArrayIndexType]</code>)           \u2013            <p>List of sets that represent the lhs side of the einsum subscript</p>"},{"location":"api_reference/#opt_einsum.paths.greedy(output)","title":"<code>output</code>","text":"(<code>ArrayIndexType</code>)           \u2013            <p>Set that represents the rhs side of the overall einsum subscript</p>"},{"location":"api_reference/#opt_einsum.paths.greedy(size_dict)","title":"<code>size_dict</code>","text":"(<code>Dict[str, int]</code>)           \u2013            <p>Dictionary of index sizes</p>"},{"location":"api_reference/#opt_einsum.paths.greedy(memory_limit)","title":"<code>memory_limit</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of elements in a temporary array</p>"},{"location":"api_reference/#opt_einsum.paths.greedy(choose_fn)","title":"<code>choose_fn</code>","text":"(<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>A function that chooses which contraction to perform from the queue</p>"},{"location":"api_reference/#opt_einsum.paths.greedy(cost_fn)","title":"<code>cost_fn</code>","text":"(<code>str</code>, default:                   <code>'memory-removed'</code> )           \u2013            <p>A function that assigns a potential contraction a cost.</p>"},{"location":"api_reference/#opt_einsumpathsbranch","title":"<code>opt_einsum.paths.branch</code>","text":""},{"location":"api_reference/#opt_einsum.paths.branch","title":"branch","text":"<pre><code>branch(\n    inputs: List[ArrayIndexType],\n    output: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n    nbranch: Optional[int] = None,\n    cutoff_flops_factor: int = 4,\n    minimize: str = \"flops\",\n    cost_fn: str = \"memory-removed\",\n) -&gt; PathType\n</code></pre>"},{"location":"api_reference/#opt_einsumpathspathoptimizer","title":"<code>opt_einsum.paths.PathOptimizer</code>","text":""},{"location":"api_reference/#opt_einsum.paths.PathOptimizer","title":"PathOptimizer","text":"<p>Base class for different path optimizers to inherit from.</p> <p>Subclassed optimizers should define a call method with signature:</p> <pre><code>def __call__(self, inputs: List[ArrayIndexType], output: ArrayIndexType, size_dict: dict[str, int], memory_limit: int | None = None) -&gt; list[tuple[int, ...]]:\n    \\\"\\\"\\\"\n    Parameters:\n        inputs: The indices of each input array.\n        outputs: The output indices\n        size_dict: The size of each index\n        memory_limit: If given, the maximum allowed memory.\n    \\\"\\\"\\\"\n    # ... compute path here ...\n    return path\n</code></pre> <p>where <code>path</code> is a list of int-tuples specifying a contraction order.</p>"},{"location":"api_reference/#opt_einsumpathsbranchbound","title":"<code>opt_einsum.paths.BranchBound</code>","text":""},{"location":"api_reference/#opt_einsum.paths.BranchBound","title":"BranchBound","text":"<pre><code>BranchBound(\n    nbranch: Optional[int] = None,\n    cutoff_flops_factor: int = 4,\n    minimize: str = \"flops\",\n    cost_fn: str = \"memory-removed\",\n)\n</code></pre> <p>               Bases: <code>PathOptimizer</code></p> <p>as well sieving by <code>memory_limit</code> and the best path found so far.</p> <p>Parameters:</p>"},{"location":"api_reference/#opt_einsum.paths.BranchBound(nbranch)","title":"<code>nbranch</code>","text":"(<code>Optional[int]</code>, default:                   <code>None</code> )           \u2013            <p>How many branches to explore at each contraction step. If None, explore all possible branches. If an integer, branch into this many paths at each step. Defaults to None.</p>"},{"location":"api_reference/#opt_einsum.paths.BranchBound(cutoff_flops_factor)","title":"<code>cutoff_flops_factor</code>","text":"(<code>int</code>, default:                   <code>4</code> )           \u2013            <p>If at any point, a path is doing this much worse than the best path found so far was, terminate it. The larger this is made, the more paths will be fully explored and the slower the algorithm. Defaults to 4.</p>"},{"location":"api_reference/#opt_einsum.paths.BranchBound(minimize)","title":"<code>minimize</code>","text":"(<code>str</code>, default:                   <code>'flops'</code> )           \u2013            <p>Whether to optimize the path with regard primarily to the total estimated flop-count, or the size of the largest intermediate. The option not chosen will still be used as a secondary criterion.</p>"},{"location":"api_reference/#opt_einsum.paths.BranchBound(cost_fn)","title":"<code>cost_fn</code>","text":"(<code>str</code>, default:                   <code>'memory-removed'</code> )           \u2013            <p>A function that returns a heuristic 'cost' of a potential contraction with which to sort candidates. Should have signature <code>cost_fn(size12, size1, size2, k12, k1, k2)</code>.</p>"},{"location":"api_reference/#opt_einsumpath_randomrandomoptimizer","title":"<code>opt_einsum.path_random.RandomOptimizer</code>","text":""},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer","title":"RandomOptimizer","text":"<pre><code>RandomOptimizer(\n    max_repeats: int = 32,\n    max_time: Optional[float] = None,\n    minimize: str = \"flops\",\n    parallel: Union[bool, Decimal, int] = False,\n    pre_dispatch: int = 128,\n)\n</code></pre> <p>               Bases: <code>PathOptimizer</code></p> <p>Base class for running any random path finder that benefits from repeated calling, possibly in a parallel fashion. Custom random optimizers should subclass this, and the <code>setup</code> method should be implemented with the following signature:</p> <pre><code>def setup(self, inputs, output, size_dict):\n    # custom preparation here ...\n    return trial_fn, trial_args\n</code></pre> <p>Where <code>trial_fn</code> itself should have the signature::</p> <pre><code>def trial_fn(r, *trial_args):\n    # custom computation of path here\n    return ssa_path, cost, size\n</code></pre> <p>Where <code>r</code> is the run number and could for example be used to seed a random number generator. See <code>RandomGreedy</code> for an example.</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>path</code>               (<code>PathType</code>)           \u2013            <p>The best path found so far.</p> </li> <li> <code>costs</code>               (<code>List[int]</code>)           \u2013            <p>The list of each trial's costs found so far.</p> </li> <li> <code>sizes</code>               (<code>List[int]</code>)           \u2013            <p>The list of each trial's largest intermediate size so far.</p> </li> </ul>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer(max_repeats)","title":"<code>max_repeats</code>","text":"(<code>int</code>, default:                   <code>32</code> )           \u2013            <p>The maximum number of repeat trials to have.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer(max_time)","title":"<code>max_time</code>","text":"(<code>Optional[float]</code>, default:                   <code>None</code> )           \u2013            <p>The maximum amount of time to run the algorithm for.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer(minimize)","title":"<code>minimize</code>","text":"(<code>str</code>, default:                   <code>'flops'</code> )           \u2013            <p>Whether to favour paths that minimize the total estimated flop-count or the size of the largest intermediate created.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer(parallel)","title":"<code>parallel</code>","text":"(<code>Union[bool, Decimal, int]</code>, default:                   <code>False</code> )           \u2013            <p>Whether to parallelize the random trials, by default <code>False</code>. If <code>True</code>, use a <code>concurrent.futures.ProcessPoolExecutor</code> with the same number of processes as cores. If an integer is specified, use that many processes instead. Finally, you can supply a custom executor-pool which should have an API matching that of the python 3 standard library module <code>concurrent.futures</code>. Namely, a <code>submit</code> method that returns <code>Future</code> objects, themselves with <code>result</code> and <code>cancel</code> methods.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer(pre_dispatch)","title":"<code>pre_dispatch</code>","text":"(<code>int</code>, default:                   <code>128</code> )           \u2013            <p>If running in parallel, how many jobs to pre-dispatch so as to avoid submitting all jobs at once. Should also be more than twice the number of workers to avoid under-subscription. Default: 128.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer.path","title":"path  <code>property</code>","text":"<pre><code>path: PathType\n</code></pre> <p>The best path found so far.</p>"},{"location":"api_reference/#opt_einsumpath_randomrandomgreedy","title":"<code>opt_einsum.path_random.RandomGreedy</code>","text":""},{"location":"api_reference/#opt_einsum.path_random.RandomGreedy","title":"RandomGreedy","text":"<pre><code>RandomGreedy(\n    cost_fn: str = \"memory-removed-jitter\",\n    temperature: float = 1.0,\n    rel_temperature: bool = True,\n    nbranch: int = 8,\n    **kwargs: Any\n)\n</code></pre> <p>               Bases: <code>RandomOptimizer</code></p> <pre><code>    with which to sort candidates. Should have signature\n    `cost_fn(size12, size1, size2, k12, k1, k2)`.\n</code></pre> <p>temperature: When choosing a possible contraction, its relative probability will be         proportional to <code>exp(-cost / temperature)</code>. Thus the larger         <code>temperature</code> is, the further random paths will stray from the normal         'greedy' path. Conversely, if set to zero, only paths with exactly the         same cost as the best at each step will be explored. rel_temperature: Whether to normalize the <code>temperature</code> at each step to the scale of         the best cost. This is generally beneficial as the magnitude of costs         can vary significantly throughout a contraction. If False, the         algorithm will end up branching when the absolute cost is low, but         stick to the 'greedy' path when the cost is high - this can also be         beneficial. nbranch: How many potential paths to calculate probability for and choose from at each step. kwargs: Supplied to RandomOptimizer.</p> <p>Attributes:</p> <ul> <li> <code>choose_fn</code>               (<code>Any</code>)           \u2013            <p>The function that chooses which contraction to take - make this a</p> </li> <li> <code>path</code>               (<code>PathType</code>)           \u2013            <p>The best path found so far.</p> </li> </ul>"},{"location":"api_reference/#opt_einsum.path_random.RandomGreedy.choose_fn","title":"choose_fn  <code>property</code>","text":"<pre><code>choose_fn: Any\n</code></pre> <p>The function that chooses which contraction to take - make this a property so that <code>temperature</code> and <code>nbranch</code> etc. can be updated between runs.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomGreedy.path","title":"path  <code>property</code>","text":"<pre><code>path: PathType\n</code></pre> <p>The best path found so far.</p>"},{"location":"api_reference/#opt_einsumpathsdynamicprogramming","title":"<code>opt_einsum.paths.DynamicProgramming</code>","text":""},{"location":"api_reference/#opt_einsum.paths.DynamicProgramming","title":"DynamicProgramming","text":"<pre><code>DynamicProgramming(\n    minimize: str = \"flops\",\n    cost_cap: Union[bool, int] = True,\n    search_outer: bool = False,\n)\n</code></pre> <p>               Bases: <code>PathOptimizer</code></p> <p>Finds the optimal path of pairwise contractions without intermediate outer products based a dynamic programming approach presented in Phys. Rev. E 90, 033315 (2014) (the corresponding preprint is publicly available at https://arxiv.org/abs/1304.6112). This method is especially well-suited in the area of tensor network states, where it usually outperforms all the other optimization strategies.</p> <p>This algorithm shows exponential scaling with the number of inputs in the worst case scenario (see example below). If the graph to be contracted consists of disconnected subgraphs, the algorithm scales linearly in the number of disconnected subgraphs and only exponentially with the number of inputs per subgraph.</p> <p>Parameters:</p>"},{"location":"api_reference/#opt_einsum.paths.DynamicProgramming(minimize)","title":"<code>minimize</code>","text":"(<code>str</code>, default:                   <code>'flops'</code> )           \u2013            <p>What to minimize: - 'flops' - minimize the number of flops - 'size' - minimize the size of the largest intermediate - 'write' - minimize the size of all intermediate tensors - 'combo' - minimize <code>flops + alpha * write</code> summed over intermediates, a default ratio of alpha=64 is used, or it can be customized with <code>f'combo-{alpha}'</code> - 'limit' - minimize <code>max(flops, alpha * write)</code> summed over intermediates, a default ratio of alpha=64 is used, or it can be customized with <code>f'limit-{alpha}'</code> - callable - a custom local cost function</p>"},{"location":"api_reference/#opt_einsum.paths.DynamicProgramming(cost_cap)","title":"<code>cost_cap</code>","text":"(<code>Union[bool, int]</code>, default:                   <code>True</code> )           \u2013            <p>How to implement cost-capping: - True - iteratively increase the cost-cap - False - implement no cost-cap at all - int - use explicit cost cap</p>"},{"location":"api_reference/#opt_einsum.paths.DynamicProgramming(search_outer)","title":"<code>search_outer</code>","text":"(<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>In rare circumstances the optimal contraction may involve an outer product, this option allows searching such contractions but may well slow down the path finding considerably on all but very small graphs.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#340-2024-09-26","title":"3.4.0 / 2024-09-26","text":"<p>NumPy has been removed from <code>opt_einsum</code> as a dependency allowing for more flexible installs.</p> <p>New Features</p> <ul> <li>#160 Migrates docs to MkDocs Material and GitHub pages hosting.</li> <li>#161 Adds Python type annotations to the code base.</li> <li>#204 Removes NumPy as a hard dependency.</li> </ul> <p>Enhancements</p> <ul> <li>#154 Prevents an infinite recursion error when the <code>memory_limit</code> was set very low for the <code>dp</code> algorithm. </li> <li>#155 Adds flake8 spell check to the doc strings</li> <li>#159 Migrates to GitHub actions for CI.</li> <li>#174 Prevents double contracts of floats in dynamic paths.</li> <li>#196 Allows <code>backend=None</code> which is equivalent to <code>backend='auto'</code></li> <li>#208 Switches to <code>ConfigParser</code> insetad of <code>SafeConfigParser</code> for Python 3.12 compatability. </li> <li>#228 <code>backend='jaxlib'</code> is now an alias for the <code>jax</code> library</li> <li>#237 Switches to <code>ruff</code> for formatting and linting.</li> <li>#238 Removes <code>numpy</code>-specific keyword args from being explicitly defined in <code>contract</code> and uses <code>**kwargs</code> instead.</li> </ul> <p>Bug Fixes</p> <ul> <li>#195 Fixes a bug where <code>dp</code> would not work for scalar-only contractions.</li> <li>#200 Fixes a bug where <code>parse_einsum_input</code> would not correctly respect shape-only contractions.</li> <li>#222 Fixes an erorr in <code>parse_einsum_input</code> where an output subscript specified multiple times was not correctly caught. </li> <li>#229 Fixes a bug where empty contraction lists in <code>PathInfo</code> would cause an error.</li> </ul>"},{"location":"changelog/#330-2020-07-19","title":"3.3.0 / 2020-07-19","text":"<p>Adds a <code>object</code> backend for optimized contractions on arbitrary Python objects.</p> <p>New Features</p> <ul> <li>#145 Adds a <code>object</code> based backend so that <code>contract(backend='object')</code> can be used on arbitrary objects such as SymPy symbols.</li> </ul> <p>Enhancements</p> <ul> <li>#140 Better error messages when the requested <code>contract</code> backend cannot be found.</li> <li>#141 Adds a check with RandomOptimizers to ensure the objects are not accidentally reused for different contractions.</li> <li>#149 Limits the <code>remaining</code> category for the <code>contract_path</code> output to only show up to 20 tensors to prevent issues with the quadratically scaling memory requirements and the number of print lines for large contractions.</li> </ul>"},{"location":"changelog/#320-2020-03-01","title":"3.2.0 / 2020-03-01","text":"<p>Small fixes for the <code>dp</code> path and support for a new mars backend.</p> <p>New Features</p> <ul> <li>#109 Adds mars backend support.</li> </ul> <p>Enhancements</p> <ul> <li>#110 New <code>auto-hq</code> and <code>'random-greedy-128'</code> paths.</li> <li>#119 Fixes several edge cases in the <code>dp</code> path.</li> </ul> <p>Bug fixes</p> <ul> <li>#127 Fixes an issue where Python 3.6 features are required while Python 3.5 is <code>opt_einsum</code>'s stated minimum version.</li> </ul>"},{"location":"changelog/#310-2019-09-30","title":"3.1.0 / 2019-09-30","text":"<p>Adds a new dynamic programming algorithm to the suite of paths.</p> <p>New Features</p> <ul> <li>#102 Adds new <code>dp</code> path.</li> </ul>"},{"location":"changelog/#300-2019-08-10","title":"3.0.0 / 2019-08-10","text":"<p>This release moves <code>opt_einsum</code> to be backend agnostic while adding support additional backends such as Jax and Autograd. Support for Python 2.7 has been dropped and Python 3.5 will become the new minimum version, a Python deprecation policy equivalent to NumPy's has been adopted.</p> <p>New Features</p> <ul> <li>#78 A new random-optimizer has been implemented which uses Boltzmann weighting to explore alternative near-minimum paths using greedy-like schemes. This provides a fairly large path performance enhancements with a linear path time overhead.</li> <li>#78 A new PathOptimizer class has been implemented to provide a framework for building new optimizers. An example is that now custom cost functions can now be provided in the greedy formalism for building custom optimizers without a large amount of additional code.</li> <li>#81 The <code>backend=\"auto\"</code> keyword has been implemented for <code>contract</code> allowing automatic detection of the correct backend to use based off provided tensors in the contraction.</li> <li>#88 Autograd and Jax support have been implemented.</li> <li>#96 Deprecates Python 2 functionality and devops improvements.</li> </ul> <p>Enhancements</p> <ul> <li>#84 The <code>contract_path</code> function can now accept shape tuples rather than full tensors.</li> <li>#84 The <code>contract_path</code> automated path algorithm decision technology has been refactored to a standalone function.</li> </ul>"},{"location":"changelog/#230-2018-12-01","title":"2.3.0 / 2018-12-01","text":"<p>This release primarily focuses on expanding the suite of available path technologies to provide better optimization characistics for 4-20 tensors while decreasing the time to find paths for 50-200+ tensors. See <code>Path Overview &lt;path_finding.html#performance-comparison&gt;</code>_ for more information.</p> <p>New Features</p> <ul> <li>#60 A new <code>greedy</code> implementation has been added which is up to two orders of magnitude faster for 200 tensors.</li> <li>#73 Adds a new <code>branch</code> path that uses <code>greedy</code> ideas to prune the <code>optimal</code> exploration space to provide a better path than <code>greedy</code> at sub <code>optimal</code> cost.</li> <li>#73 Adds a new <code>auto</code> keyword to the <code>opt_einsum.contract</code> <code>path</code> option. This keyword automatically chooses the best path technology that takes under 1ms to execute.</li> </ul> <p>Enhancements</p> <ul> <li>#61 The <code>opt_einsum.contract</code> <code>path</code> keyword has been changed to <code>optimize</code> to more closely match NumPy. <code>path</code> will be deprecated in the future.</li> <li>#61 The <code>opt_einsum.contract_path</code> now returns a <code>opt_einsum.contract.PathInfo</code> object that can be queried for the scaling, flops, and intermediates of the path. The print representation of this object is identical to before.</li> <li>#61 The default <code>memory_limit</code> is now unlimited by default based on community feedback.</li> <li>#66 The Torch backend will now use <code>tensordot</code> when using a version of Torch which includes this functionality.</li> <li>#68 Indices can now be any hashable object when provided in the <code>\"Interleaved Input\" &lt;input_format.html#interleaved-input&gt;</code>_ syntax.</li> <li>#74 Allows the default <code>transpose</code> operation to be overridden to take advantage of more advanced tensor transpose libraries.</li> <li>#73 The <code>optimal</code> path is now significantly faster.</li> <li>#81 A documentation pass for v3.0.</li> </ul> <p>Bug fixes</p> <ul> <li>#72 Fixes the <code>\"Interleaved Input\" &lt;input_format.html#interleaved-input&gt;</code>_ syntax and adds documentation.</li> </ul>"},{"location":"changelog/#220-2018-07-29","title":"2.2.0 / 2018-07-29","text":"<p>New Features</p> <ul> <li>#48 Intermediates can now be shared between contractions, see here for more details.</li> <li>#53 Intermediate caching is thread safe.</li> </ul> <p>Enhancements</p> <ul> <li>#48 Expressions are now mapped to non-unicode index set so that unicode input is support for all backends.</li> <li>#54 General documentation update.</li> </ul> <p>Bug fixes</p> <ul> <li>#41 PyTorch indices are mapped back to a small a-z subset valid for PyTorch's einsum implementation.</li> </ul>"},{"location":"changelog/#213-2018-8-23","title":"2.1.3 / 2018-8-23","text":"<p>Bug fixes</p> <ul> <li>Fixes unicode issue for large numbers of tensors in Python 2.7.</li> <li>Fixes unicode install bug in README.md.</li> </ul>"},{"location":"changelog/#212-2018-8-16","title":"2.1.2 / 2018-8-16","text":"<p>Bug fixes</p> <ul> <li>Ensures <code>versioneer.py</code> is in MANIFEST.in for a clean pip install.</li> </ul>"},{"location":"changelog/#211-2018-8-15","title":"2.1.1 / 2018-8-15","text":"<p>Bug fixes</p> <ul> <li>Corrected Markdown display on PyPi.</li> </ul>"},{"location":"changelog/#210-2018-8-15","title":"2.1.0 / 2018-8-15","text":"<p><code>opt_einsum</code> continues to improve its support for additional backends beyond NumPy with PyTorch.</p> <p>We have also published the opt_einsum package in the Journal of Open Source Software. If you use this package in your work, please consider citing us!</p> <p>New features</p> <ul> <li>PyTorch backend support</li> <li>Tensorflow eager-mode execution backend support</li> </ul> <p>Enhancements</p> <ul> <li>Intermediate tensordot-like expressions are now ordered to avoid transposes.</li> <li>CI now uses conda backend to better support GPU and tensor libraries.</li> <li>Now accepts arbitrary unicode indices rather than a subset.</li> <li>New auto path option which switches between optimal and greedy at four tensors.</li> </ul> <p>Bug fixes</p> <ul> <li>Fixed issue where broadcast indices were incorrectly locked out of tensordot-like evaluations even after their dimension was broadcast.</li> </ul>"},{"location":"changelog/#201-2018-6-28","title":"2.0.1 / 2018-6-28","text":"<p>New Features</p> <ul> <li>Allows unlimited Unicode indices.</li> <li>Adds a Journal of Open-Source Software paper.</li> <li>Minor documentation improvements.</li> </ul>"},{"location":"changelog/#200-2018-5-17","title":"2.0.0 / 2018-5-17","text":"<p><code>opt_einsum</code> is a powerful tensor contraction order optimizer for NumPy and related ecosystems.</p> <p>New Features</p> <ul> <li>Expressions can be precompiled so that the expression optimization need not happen multiple times.</li> <li>The greedy order optimization algorithm has been tuned to be able to handle hundreds of tensors in several seconds.</li> <li>Input indices can now be unicode so that expressions can have many thousands of indices.</li> <li>GPU and distributed computing backends have been added such as Dask, TensorFlow, CUPy, Theano, and Sparse.</li> </ul> <p>Bug Fixes</p> <ul> <li>An error affecting cases where opt_einsum mistook broadcasting operations for matrix multiply has been fixed.</li> <li>Most error messages are now more expressive.</li> </ul>"},{"location":"changelog/#100-2016-10-14","title":"1.0.0 / 2016-10-14","text":"<p>Einsum is a very powerful function for contracting tensors of arbitrary dimension and index. However, it is only optimized to contract two terms at a time resulting in non-optimal scaling for contractions with many terms. Opt_einsum aims to fix this by optimizing the contraction order which can lead to arbitrarily large speed ups at the cost of additional intermediate tensors.</p> <p>Opt_einsum is also implemented into the np.einsum function as of NumPy v1.12.</p> <p>New Features</p> <ul> <li>Tensor contraction order optimizer.</li> <li><code>opt_einsum.contract</code> as a drop-in replacement for <code>numpy.einsum</code>.</li> </ul>"},{"location":"examples/dask_reusing_intermediaries/","title":"Reusing Intermediaries with Dask","text":"<p>Dask provides a computational framework where arrays and the computations on them are built up into a 'task graph' before computation. Since :mod:<code>opt_einsum</code> is compatible with <code>dask</code> arrays this means that multiple contractions can be built into the same task graph, which then automatically reuses any shared arrays and contractions.</p> <p>For example, imagine the two expressions:</p> <pre><code>contraction1 = 'ab,dca,eb,cde'\ncontraction2 = 'ab,cda,eb,cde'\nsizes = {l: 10 for l in 'abcde'}\n</code></pre> <p>The contraction <code>'ab,eb'</code> is shared between them and could only be done once. First, let's set up some <code>numpy</code> arrays:</p> <pre><code>terms1, terms2 = contraction1.split(','), contraction2.split(',')\nterms = set((*terms1, *terms2))\nterms\n#&gt; {'ab', 'cda', 'cde', 'dca', 'eb'}\n\nimport numpy as np\nnp_arrays = {s: np.random.randn(*(sizes[c] for c in s)) for s in terms}\n# filter the arrays needed for each expression\nnp_ops1 = [np_arrays[s] for s in terms1]\nnp_ops2 = [np_arrays[s] for s in terms2]\n</code></pre> <p>Typically we would compute these expressions separately:</p> <pre><code>oe.contract(contraction1, *np_ops1)\n#&gt; array(114.78314052)\n\noe.contract(contraction2, *np_ops2)\n#&gt; array(-75.55902751)\n</code></pre> <p>However, if we use dask arrays we can combine the two operations, so let's set those up:</p> <pre><code>import dask.array as da\nda_arrays = {s: da.from_array(np_arrays[s], chunks=1000, name=s) for s in inputs}\nda_arrays\n#&gt; {'ab': dask.array&lt;ab, shape=(10, 10), dtype=float64, chunksize=(10, 10)&gt;,\n#&gt;  'cda': dask.array&lt;cda, shape=(10, 10, 10), dtype=float64, chunksize=(10, 10, 10)&gt;,\n#&gt;  'cde': dask.array&lt;cde, shape=(10, 10, 10), dtype=float64, chunksize=(10, 10, 10)&gt;,\n#&gt;  'dca': dask.array&lt;dca, shape=(10, 10, 10), dtype=float64, chunksize=(10, 10, 10)&gt;,\n#&gt;  'eb': dask.array&lt;eb, shape=(10, 10), dtype=float64, chunksize=(10, 10)&gt;}\n\nda_ops1 = [da_arrays[s] for s in terms1]\nda_ops2 = [da_arrays[s] for s in terms2]\n</code></pre> <p>Note <code>chunks</code> is a required argument relating to how the arrays are stored (see array-creation). Now we can perform the contraction:</p> <pre><code># these won't be immediately evaluated\ndy1 = oe.contract(contraction1, *da_ops1, backend='dask')\ndy2 = oe.contract(contraction2, *da_ops2, backend='dask')\n\n# wrap them in delayed to combine them into the same computation\nfrom dask import delayed\ndy = delayed([dy1, dy2])\ndy\n#&gt; Delayed('list-3af82335-b75e-47d6-b800-68490fc865fd')\n</code></pre> <p>As suggested by the name <code>Delayed</code>, we have a placeholder for the result so far. When we want to perform the computation we can call:</p> <pre><code>dy.compute()\n#&gt; [114.78314052155015, -75.55902750513113]\n</code></pre> <p>The above matches the canonical numpy result. The computation can even be handled by various schedulers - see scheduling. Finally, to check we are reusing intermediaries, we can view the task graph generated for the computation:</p> <pre><code>dy.visualize(optimize_graph=True)\n</code></pre> <p></p> <p>Note</p> <p>For sharing intermediates with other backends see Sharing Intermediates. Dask graphs are particularly useful for reusing intermediates beyond just contractions and can allow additional parallelization.</p>"},{"location":"examples/large_expr_with_greedy/","title":"Large Expressions with Greedy","text":"<p>Using the greedy method allows the contraction of hundreds of tensors. Here's an example from quantum of computing the inner product between two 'Matrix Product States'. Graphically, if we represent each tensor as an <code>O</code>, give it the same number of 'legs' as it has indices, and join those legs when that index is summed with another tensor, we get an expression for <code>n</code> particles that looks like:</p> <pre><code>O-O-O-O-O-O-     -O-O-O-O-O-O\n| | | | | |  ...  | | | | | |\nO-O-O-O-O-O-     -O-O-O-O-O-O\n\n0 1 2 3 4 5 ........... n-2 n-1\n</code></pre> <p>The meaning of this is not that important other than its a large, useful contraction. For <code>n=100</code> it involves 200 different tensors and about 300 unique indices. With this many indices it can be useful to generate them with the function <code>opt_einsum.parser.get_symbol</code>.</p>"},{"location":"examples/large_expr_with_greedy/#setup-the-string","title":"Setup the string","text":"<pre><code>import numpy as np\nimport opt_einsum as oe\n\nn = 100\nphys_dim = 3\nbond_dim = 10\n\n# start with first site\n# O--\n# |\n# O--\neinsum_str = \"ab,ac,\"\n\nfor i in range(1, n - 1):\n    # set the upper left/right, middle and lower left/right indices\n    # --O--\n    #   |\n    # --O--\n    j = 3 * i\n    ul, ur, m, ll, lr = (oe.get_symbol(i)\n                         for i in (j - 1, j + 2, j, j - 2, j + 1))\n    einsum_str += \"{}{}{},{}{}{},\".format(m, ul, ur, m, ll, lr)\n\n# finish with last site\n# --O\n#   |\n# --O\ni = n - 1\nj = 3 * i\nul, m, ll, =  (oe.get_symbol(i) for i in (j - 1, j, j - 2))\neinsum_str += \"{}{},{}{}\".format(m, ul, m, ll)\n</code></pre>"},{"location":"examples/large_expr_with_greedy/#generate-the-shapes","title":"Generate the shapes","text":"<pre><code>def gen_shapes():\n    yield (phys_dim, bond_dim)\n    yield (phys_dim, bond_dim)\n    for i in range(1, n - 1):\n        yield(phys_dim, bond_dim, bond_dim)\n        yield(phys_dim, bond_dim, bond_dim)\n    yield (phys_dim, bond_dim)\n    yield (phys_dim, bond_dim)\n\nshapes = tuple(gen_shapes())\n</code></pre> <p>Let's time how long it takes to generate the expression (<code>'greedy'</code> is used by default, and we turn off the <code>memory_limit</code>):</p> <pre><code>%timeit expr = oe.contract_expression(einsum_str, *shapes, memory_limit=-1)\n#&gt; 76.2 ms \u00b1 1.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> <p>This is pretty manageable, though we might want to think about splitting the expression up if we go a lot bigger. Importantly, we can then use this repeatedly with any set of matching arrays:</p> <pre><code>arrays = [np.random.randn(*shp) / 4 for shp in shapes]\nexpr(*arrays)\n#&gt; array(23.23628116)\n\narrays = [np.random.randn(*shp) / 4 for shp in shapes]\nexpr(*arrays)\n#&gt; array(-12.21091879)\n</code></pre>"},{"location":"examples/large_expr_with_greedy/#full-path","title":"Full path","text":"<p>And if we really want we can generate the full contraction path info:</p> <pre><code>print(oe.contract_path(einsum_str, *arrays, memory_limit=-1)[1])\n#&gt;   Complete contraction:  ab,ac,dcf,dbe,gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b2\u01b5,\u01b3\u01b1\u01b4,\u01b6\u01b5,\u01b6\u01b4-&gt;\n#&gt;          Naive scaling:  298\n#&gt;      Optimized scaling:  5\n#&gt;       Naive FLOP count:  1.031e+248\n#&gt;   Optimized FLOP count:  1.168e+06\n#&gt;    Theoretical speedup:  88264689284468460017580864156865782413140936705854966013600065426858041248009637246968036807489558012989638169986640870276510490846199301907401763236976204166215471281505344088317454144870323271826022036197984172898402324699098341524952317952.000\n#&gt;   Largest intermediate:  3.000e+02 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    4           TDOT            dbe,ab-&gt;ade ac,dcf,gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b2\u01b5,\u01b3\u01b1\u01b4,\u01b6\u01b5,\u01b6\u01b4,ade-&gt;\n#&gt;    4           TDOT            dcf,ac-&gt;adf gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b2\u01b5,\u01b3\u01b1\u01b4,\u01b6\u01b5,\u01b6\u01b4,ade,adf-&gt;\n#&gt;    4           GEMM            \u01b6\u01b5,\u01b3\u01b2\u01b5-&gt;\u01b3\u01b6\u01b2 gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b1\u01b4,\u01b6\u01b4,ade,adf,\u01b3\u01b6\u01b2-&gt;\n#&gt;    4           GEMM            \u01b6\u01b4,\u01b3\u01b1\u01b4-&gt;\u01b3\u01b6\u01b1 gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,ade,adf,\u01b3\u01b6\u01b2,\u01b3\u01b6\u01b1-&gt;\n#&gt;    5           TDOT          ade,geh-&gt;adgh gfi,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,adf,\u01b3\u01b6\u01b2,\u01b3\u01b6\u01b1,adgh-&gt;\n#&gt; \n#&gt;    ...\n#&gt; \n#&gt;    4           TDOT            \u011e\u011f,\u0120\u011f\u0122-&gt;\u0120\u011e\u0122                  \u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0124\u0125,\u0120\u011e\u0122-&gt;\n#&gt;    4           GEMM            \u0120\u011e\u0122,\u0120\u011e\u0121-&gt;\u0121\u0122                       \u0123\u0122\u0125,\u0123\u0121\u0124,\u0124\u0125,\u0121\u0122-&gt;\n#&gt;    4           GEMM            \u0124\u0125,\u0123\u0122\u0125-&gt;\u0123\u0122\u0124                          \u0123\u0121\u0124,\u0121\u0122,\u0123\u0122\u0124-&gt;\n#&gt;    4           TDOT            \u0123\u0122\u0124,\u0123\u0121\u0124-&gt;\u0121\u0122                               \u0121\u0122,\u0121\u0122-&gt;\n#&gt;    2            DOT                \u0121\u0122,\u0121\u0122-&gt;                                    -&gt;\n</code></pre> <p>Where we can see the speedup over a naive einsum is about <code>10^241</code>, not bad!</p>"},{"location":"getting_started/backends/","title":"Backends &amp; GPU Support","text":"<p><code>opt_einsum</code> is largely agnostic to the type of n-dimensional arrays (tensors) it uses, since finding the contraction path only relies on getting the shape attribute of each array supplied. It can perform the underlying tensor contractions with various libraries. In fact, any library that provides a <code>numpy.tensordot</code> and <code>numpy.transpose</code> implementation can perform most normal contractions. However, certain special functionalities such as axes reduction are reliant on a <code>numpy.einsum</code> implementation. The following is a brief overview of libraries which have been tested with <code>opt_einsum</code>:</p> <ul> <li>tensorflow: compiled tensor expressions   that can run on GPU.</li> <li>theano: compiled tensor   expressions that can run on GPU.</li> <li>cupy: numpy-like api for GPU tensors.</li> <li>dask: larger-than-memory tensor   computations, distributed scheduling, and potential reuse of   intermediaries.</li> <li>sparse: sparse tensors.</li> <li>pytorch: numpy-like api for GPU tensors.</li> <li>autograd: automatic derivative   computation for tensor expressions</li> <li>jax: compiled GPU tensor expressions   including <code>autograd</code>-like functionality</li> </ul> <p>Note</p> <p>For a contraction to be possible without using a backend einsum, it must satisfy the following rule: in the full expression (including output indices) each index must appear twice. In other words, each dimension must be either contracted with one other dimension or left alone.</p>"},{"location":"getting_started/backends/#backend-agnostic-contractions","title":"Backend agnostic contractions","text":"<p>The automatic backend detection will be detected based on the first supplied array (default), this can be overridden by specifying the correct <code>backend</code> argument for the type of arrays supplied when calling <code>opt_einsum.contract</code>. For example, if you had a library installed called <code>'foo'</code> which provided an <code>numpy.ndarray</code> like object with a <code>.shape</code> attribute as well as <code>foo.tensordot</code> and <code>foo.transpose</code> then you could contract them with something like:</p> <pre><code>contract(einsum_str, *foo_arrays, backend='foo')\n</code></pre> <p>Behind the scenes <code>opt_einsum</code> will find the contraction path, perform pairwise contractions using e.g. <code>foo.tensordot</code> and finally return the canonical type those functions return.</p>"},{"location":"getting_started/backends/#dask","title":"Dask","text":"<p>dask is an example of a library which satisfies these requirements. For example:</p> <pre><code>import opt_einsum as oe\nimport dask.array as da\nshapes = (3, 200), (200, 300), (300, 4)\ndxs = [da.random.normal(0, 1, shp, chunks=(100, 100)) for shp in shapes]\ndxs\n#&gt; [dask.array&lt;da.random.normal, shape=(3, 200), dtype=float64, chunksize=(3, 100)&gt;,\n#&gt;  dask.array&lt;da.random.normal, shape=(200, 300), dtype=float64, chunksize=(100, 100)&gt;,\n#&gt;  dask.array&lt;da.random.normal, shape=(300, 4), dtype=float64, chunksize=(100, 4)&gt;]\n\n\ndy = oe.contract(\"ab,bc,cd\", *dxs)  # will infer backend='dask'\ndy\n#&gt; dask.array&lt;transpose, shape=(3, 4), dtype=float64, chunksize=(3, 4)&gt;\n\ndy.compute()\n#&gt; array([[ 470.71404665,    2.44931372,  -28.47577265,  424.37716615],\n#&gt;        [  64.38328345, -287.40753131,  144.46515642,  324.88169821],\n#&gt;        [-142.07153553, -180.41739259,  125.0973783 , -239.16754541]])\n</code></pre> <p>In this case, dask arrays in = dask array out, since dask arrays have a shape attribute, and <code>opt_einsum</code> can find <code>dask.array.tensordot</code> and <code>dask.array.transpose</code>.</p>"},{"location":"getting_started/backends/#sparse","title":"Sparse","text":"<p>The sparse library also fits the requirements and is supported. An example:</p> <pre><code>import sparse as sp\nshapes = (3, 200), (200, 300), (300, 4)\nsxs = [sp.random(shp) for shp in shapes]\nsxs\n#&gt; [&lt;COO: shape=(3, 200), dtype=float64, nnz=6, sorted=False, duplicates=True&gt;,\n#&gt;  &lt;COO: shape=(200, 300), dtype=float64, nnz=600, sorted=False, duplicates=True&gt;,\n#&gt;  &lt;COO: shape=(300, 4), dtype=float64, nnz=12, sorted=False, duplicates=True&gt;]\n\noe.contract(\"ab,bc,cd\", *sxs)\n#&gt; &lt;COO: shape=(3, 4), dtype=float64, nnz=0, sorted=False, duplicates=False&gt;\n</code></pre>"},{"location":"getting_started/backends/#autograd","title":"Autograd","text":"<p>The autograd library is a drop-in for <code>numpy</code> that can automatically compute the gradients of array expressions. <code>opt_einsum</code> automatically dispatches the <code>autograd</code> arrays correctly, enabling a simple way to compute gradients of tensor contractions:</p> <pre><code>import numpy as np\nimport autograd\nshapes = [(2, 3), (3, 4), (4, 2)]\nx, y, z = [np.random.rand(*s) for s in shapes]\n\n# make single arg function as autograd takes derivative of first arg\ndef foo(xyz):\n   return oe.contract('ij,jk,ki-&gt;', *xyz)\n\nfoo([x, y, z])\n#&gt; array(4.90422159)\n\n# wrap foo with autograd to compute gradients instead\ndfoo = autograd.grad(foo)\ndx, dy, dz = dfoo(arrays)\ndx, dy, dz\n#&gt; (array([[1.10056194, 1.25078356, 1.48211494],\n#&gt;         [1.38945961, 1.5572077 , 1.65234003]]),\n#&gt;  array([[0.41710717, 0.63202881, 0.84573502, 0.95069975],\n#&gt;         [0.42706777, 0.73630994, 0.99328938, 0.77415267],\n#&gt;         [0.40773334, 0.61693475, 0.82545726, 0.93132302]]),\n#&gt;  array([[0.78747828, 1.28979012],\n#&gt;         [1.26051133, 1.48835538],\n#&gt;         [0.46896666, 0.55003072],\n#&gt;         [1.10840828, 1.16722494]]))\n</code></pre>"},{"location":"getting_started/backends/#jax","title":"Jax","text":"<p>jax is itself a drop-in for <code>autograd</code>, that additionally uses XLA to compile the expressions, particularly for the GPU. Using it with <code>opt_einsum</code> is very simple:</p> <pre><code>import jax\n# generate a compiled version of the above function\njit_foo = jax.jit(foo)\njit_foo([x, y, z])\n#&gt; DeviceArray(4.9042215, dtype=float32)\n\n# generate a compiled version of the gradient function\njit_dfoo = jax.jit(jax.grad(foo))\njit_dfoo([x, y, z])\n#&gt; [DeviceArray([[1.10056198, 1.25078356, 1.48211491],\n#&gt;               [1.38945973, 1.5572077, 1.65234005]], dtype=float32),\n#&gt;  DeviceArray([[0.41710716, 0.63202882, 0.84573501, 0.95069975],\n#&gt;               [0.42706776, 0.73630995, 0.99328935, 0.7741527 ],\n#&gt;               [0.40773335, 0.61693472, 0.82545722, 0.93132305]],\n#&gt;              dtype=float32),\n#&gt;  DeviceArray([[0.78747827, 1.28979015],\n#&gt;               [1.2605114 , 1.4883554 ],\n#&gt;               [0.46896666, 0.55003077],\n#&gt;               [1.10840821, 1.16722488]], dtype=float32)]\n</code></pre> <p>Note</p> <p><code>jax</code> defaults to converting all arrays to single precision. This behaviour can be changed by running <code>from jax.config import config; config.update(\"jax_enable_x64\", True)</code> before it has been imported and used at all.</p>"},{"location":"getting_started/backends/#special-gpu-backends-for-numpy-arrays","title":"Special (GPU) backends for numpy arrays","text":"<p>A particular case is if numpy arrays are required for the input and output, however, a more performant backend is required such as performing the contraction on a GPU. Unless the specified backend works on numpy arrays, this requires converting to and from the backend array type. Currently <code>opt_einsum</code> can handle this automatically for:</p> <ul> <li>tensorflow</li> <li>theano</li> <li>cupy</li> <li>pytorch</li> <li>jax</li> </ul> <p>all of which offer GPU support. Since <code>tensorflow</code> and <code>theano</code> both require compiling the expression, this functionality is encapsulated in generating a <code>opt_einsum.ContractExpression</code> using <code>opt_einsum.contract_expression</code>, which can then be called using numpy arrays whilst specifying <code>backend='tensorflow'</code> etc. Additionally, if arrays are marked as <code>constant</code> (see <code>constants-section</code>), then these arrays will be kept on the device for optimal performance.</p>"},{"location":"getting_started/backends/#theano","title":"Theano","text":"<p>If <code>theano</code> is installed, using it as backend is as simple as specifying <code>backend='theano'</code>:</p> <pre><code>shapes = (3, 200), (200, 300), (300, 4)\nexpr = oe.contract_expression(\"ab,bc,cd\", *shapes)\nexpr\n#&gt; &lt;ContractExpression('ab,bc,cd')&gt;\n\nimport numpy as np\n# GPU advantage mainly for low precision numbers\nxs = [np.random.randn(*shp).astype(np.float32) for shp in shapes]\nexpr(*xs, backend='theano')  # might see some fluff on first run\n#&gt; array([[ 129.28352  , -128.00702  , -164.62917  , -335.11682  ],\n#&gt;        [-462.52344  , -121.12657  ,  -67.847626 ,  624.5457   ],\n#&gt;        [   5.2838974,   36.441578 ,   81.62851  ,  703.1576   ]],\n#&gt;       dtype=float32)\n</code></pre> <p>Note that you can still supply <code>theano.tensor.TensorType</code> directly to <code>opt_einsum</code> (with <code>backend='theano'</code>), and it will return the relevant <code>theano</code> type.</p>"},{"location":"getting_started/backends/#tensorflow","title":"Tensorflow","text":"<p>To run the expression with tensorflow, you need to register a default session:</p> <pre><code>import tensorflow as tf\nsess = tf.Session()\n\nwith sess.as_default():\n    out = expr(*xs, backend='tensorflow')\n\nout\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n</code></pre> <p>Note that you can still supply this expression with, for example, a <code>tensorflow.placeholder</code> using <code>backend='tensorflow'</code>, and then no conversion would take place, instead you'd get a <code>tensorflow.Tensor</code> back.</p> <p>Version 1.9 of tensorflow also added support for eager execution of computations. If compilation of the contraction expression tensorflow graph is taking a substantial amount of time up then it can be advantageous to use this, especially since tensor contractions are quite compute-bound. This is achieved by running the following snippet:</p> <pre><code>import tensorflow as tf\ntf.enable_eager_execution()\n</code></pre> <p>After which <code>opt_einsum</code> will automatically detect eager mode if <code>backend='tensorflow'</code> is supplied to a <code>opt_einsum.ContractExpression</code>.</p>"},{"location":"getting_started/backends/#pytorch-cupy","title":"Pytorch &amp; Cupy","text":"<p>Both pytorch and cupy offer numpy-like, GPU-enabled arrays which execute eagerly rather than requiring any compilation. If they are installed, no steps are required to utilize them other than specifying the <code>backend</code> keyword:</p> <pre><code>expr(*xs, backend='torch')\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n\nexpr(*xs, backend='cupy')\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n</code></pre> <p>And as with the other GPU backends, if raw <code>cupy</code> or <code>pytorch</code> arrays are supplied the returned array will be of the same type, with no conversion to or from <code>numpy</code> arrays.</p>"},{"location":"getting_started/backends/#jax_1","title":"Jax","text":"<p>jax, as introduced above, can compile tensor functions, in doing so often achieving better performance. <code>opt_einsum</code> expressions can handle this behind the scenes, so again just the <code>backend</code> keyword needs to be supplied:</p> <pre><code>expr(*xs, backend='jax')\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n</code></pre>"},{"location":"getting_started/backends/#contracting-arbitrary-objects","title":"Contracting arbitrary objects","text":"<p>There is one more explicit backend that can handle arbitrary arrays of objects, so long the objects themselves just support multiplication and addition ( <code>__mul__</code> and <code>__add__</code> dunder methods respectively). Use it by supplying <code>backend='object'</code>.</p> <p>For example, imagine we want to perform a contraction of arrays made up of sympy symbols:</p> <pre><code>import opt_einsum as oe\nimport numpy as np\nimport sympy\n\n# define the symbols\na, b, c, d, e, f, g, h, i, j, k, l = [sympy.symbols(oe.get_symbol(i)) for i in range(12)]\na * b + c * d\n\ud835\udc51\n\n# define the tensors (you might explicitly specify `dtype=object`)\nX = np.array([[a, b], [c, d]])\nY = np.array([[e, f], [g, h]])\nZ = np.array([[i, j], [k, l]])\n\n# contract the tensors!\noe.contract('uv,vw,wu-&gt;u', X, Y, Z, backend='object')\n# array([i*(a*e + b*g) + k*(a*f + b*h), j*(c*e + d*g) + l*(c*f + d*h)],\n#       dtype=object)\n</code></pre> <p>There are a few things to note here:</p> <ul> <li>The returned array is a <code>numpy.ndarray</code> but since it has <code>dtype=object</code>   it can really hold any python objects</li> <li>We had to explicitly use <code>backend='object'</code>, since <code>numpy.einsum</code>   would have otherwise been dispatched to, which can't handle <code>dtype=object</code>   (though <code>numpy.tensordot</code> in fact can)</li> <li>Although an optimized pairwise contraction order is used, the looping in each   single contraction is performed in python so performance will be   drastically lower than for numeric dtypes!</li> </ul>"},{"location":"getting_started/input_format/","title":"Input Format","text":"<p>The <code>opt_einsum</code> package was originally designed as a drop-in replacement for the <code>np.einsum</code> function and supports all input formats that <code>np.einsum</code> supports. There are two styles of input accepted, a basic introduction to which can be found in the documentation for <code>numpy.einsum</code>. In addition to this, <code>opt_einsum</code> extends the allowed index labels to unicode or arbitrary hashable, comparable objects in order to handle large contractions with many indices.</p>"},{"location":"getting_started/input_format/#equation-input","title":"'Equation' Input","text":"<p>As with <code>numpy.einsum</code>, here you specify an equation as a string, followed by the array arguments:</p> <pre><code>import opt_einsum as oe\neq = 'ijk,jkl-&gt;li'\nx, y = np.random.rand(2, 3, 4), np.random.rand(3, 4, 5)\nz = oe.contract(eq, x, y)\nz.shape\n#&gt; (5, 2)\n</code></pre> <p>However, in addition to the standard alphabet, <code>opt_einsum</code> also supports unicode characters:</p> <pre><code>eq = \"\u03b1\u03b2\u03b3,\u03b2\u03b3\u03b4-&gt;\u03b4\u03b1\"\noe.contract(eq, x, y).shape\n#&gt; (5, 2)\n</code></pre> <p>This enables access to thousands of possible index labels. One way to access these programmatically is through the function <code>get_symbols</code>:</p> <pre><code>oe.get_symbol(805)\n#&gt; '\u03b1'\n</code></pre> <p>which maps an <code>int</code> to a unicode characater. Note that as with <code>numpy.einsum</code> if the output is not specified with <code>-&gt;</code> it will default to the sorted order of all indices appearing once:</p> <pre><code>eq = \"\u03b1\u03b2\u03b3,\u03b2\u03b3\u03b4\"  # \"-&gt;\u03b1\u03b4\" is implicit\noe.contract(eq, x, y).shape\n#&gt; (2, 5)\n</code></pre>"},{"location":"getting_started/input_format/#interleaved-input","title":"'Interleaved' Input","text":"<p>The other input format is to 'interleave' the array arguments with their index labels ('subscripts') in pairs, optionally specifying the output indices as a final argument. As with <code>numpy.einsum</code>, integers are allowed as these index labels:</p> <pre><code>oe.contract(x, [1, 2, 3], y, [2, 3, 4], [4, 1]).shape\n#&gt; (5, 2)\n</code></pre> <p>with the default output order again specified by the sorted order of indices appearing once. However, unlike <code>numpy.einsum</code>, in <code>opt_einsum</code> you can also put anything hashable and comparable such as <code>str</code> in the subscript list. A simple example of this syntax is:</p> <pre><code>x, y, z = np.ones((1, 2)), np.ones((2, 2)), np.ones((2, 1))\noe.contract(x, ('left', 'bond1'), y, ('bond1', 'bond2'), z, ('bond2', 'right'), ('left', 'right'))\n#&gt; array([[4.]])\n</code></pre> <p>The subscripts need to be hashable so that <code>opt_einsum</code> can efficiently process them, and they should also be comparable so as to allow a default sorted output. For example:</p> <pre><code>x = np.array([[0, 1], [2, 0]])\n\n# original matrix\noe.contract(x, (0, 1))\n#&gt; array([[0, 1],\n#&gt;        [2, 0]])\n\n# the transpose\noe.contract(x, (1, 0))\n#&gt; array([[0, 2],\n#&gt;        [1, 0]])\n\n# original matrix, consistent behavior\noe.contract(x, ('a', 'b'))\n#&gt; array([[0, 1],\n#&gt;        [2, 0]])\n\n# the transpose, consistent behavior\n&gt;&gt;&gt; oe.contract(x, ('b', 'a'))\n#&gt; array([[0, 2],\n#&gt;        [1, 0]])\n\n# relative sequence undefined, can't determine output\n&gt;&gt;&gt; oe.contract(x, (0, 'a'))\n#&gt; TypeError: For this input type lists must contain either Ellipsis\n#&gt; or hashable and comparable object (e.g. int, str)\n</code></pre>"},{"location":"getting_started/install/","title":"Install opt_einsum","text":"<p>You can install <code>opt_einsum</code> with <code>conda</code>, with <code>pip</code>, or by installing from source.</p>"},{"location":"getting_started/install/#conda","title":"Conda","text":"<p>You can update <code>opt_einsum</code> using <code>conda</code>:</p> <pre><code>conda install opt_einsum -c conda-forge\n</code></pre> <p>This installs <code>opt_einsum</code> and the NumPy dependency.</p> <p>The <code>opt_einsum</code> package is maintained on the conda-forge channel.</p>"},{"location":"getting_started/install/#pip","title":"Pip","text":"<p>To install <code>opt_einsum</code> with <code>pip</code> there are a few options, depending on which dependencies you would like to keep up to date:</p> <ul> <li><code>pip install opt_einsum</code></li> </ul>"},{"location":"getting_started/install/#install-from-source","title":"Install from Source","text":"<p>To install opt_einsum from source, clone the repository from github:</p> <pre><code>git clone https://github.com/dgasmith/opt_einsum.git\ncd opt_einsum\npython setup.py install\n</code></pre> <p>or use <code>pip</code> locally if you want to install all dependencies as well::</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting_started/install/#test","title":"Test","text":"<p>Test <code>opt_einsum</code> with <code>py.test</code>:</p> <pre><code>cd opt_einsum\npytest\n</code></pre>"},{"location":"getting_started/reusing_paths/","title":"Reusing Paths","text":"<p>If you expect to use a particular contraction repeatedly, it can make things simpler and more efficient not to compute the path each time. Instead, supplying <code>opt_einsum.contract_expression</code> with the contraction string and the shapes of the tensors generates a <code>opt_einsum.ContractExpression</code> which can then be repeatedly called with any matching set of arrays. For example:</p> <pre><code>my_expr = oe.contract_expression(\"abc,cd,dbe-&gt;ea\", (2, 3, 4), (4, 5), (5, 3, 6))\nprint(my_expr)\n#&gt; &lt;ContractExpression('abc,cd,dbe-&gt;ea')&gt;\n#&gt;   1.  'dbe,cd-&gt;bce' [GEMM]\n#&gt;   2.  'bce,abc-&gt;ea' [GEMM]\n</code></pre> <p>The <code>ContractExpression</code> can be called with 3 arrays that match the original shapes without having to recompute the path:</p> <pre><code>x, y, z = (np.random.rand(*s) for s in [(2, 3, 4), (4, 5), (5, 3, 6)])\nmy_expr(x, y, z)\n#&gt; array([[ 3.08331541,  4.13708916],\n#&gt;        [ 2.92793729,  4.57945185],\n#&gt;        [ 3.55679457,  5.56304115],\n#&gt;        [ 2.6208398 ,  4.39024187],\n#&gt;        [ 3.66736543,  5.41450334],\n#&gt;        [ 3.67772272,  5.46727192]])\n</code></pre> <p>Note that few checks are performed when calling the expression, and while it will work for a set of arrays with the same ranks as the original shapes but differing sizes, it might no longer be optimal.</p>"},{"location":"getting_started/reusing_paths/#specifying-constants","title":"Specifying Constants","text":"<p>Often one generates contraction expressions where some of the tensor arguments will remain constant across many calls. <code>opt_einsum.contract_expression</code> allows you to specify the indices of these constant arguments, allowing <code>opt_einsum</code> to build and then reuse as many constant contractions as possible.</p> <p>Take for example the equation:</p> <pre><code>eq = \"ij,jk,kl,lm,mn-&gt;ni\"\n</code></pre> <p>where we know that only the first and last tensors will vary between calls. We can specify this by marking the middle three as constant - we then need to supply the actual arrays rather than just the shapes to <code>opt_einsum.contract_expression</code>:</p> <pre><code>#           A       B       C       D       E\nshapes = [(9, 5), (5, 5), (5, 5), (5, 5), (5, 8)]\n\n# mark the middle three arrays as constant\nconstants = [1, 2, 3]\n\n# generate the constant arrays\nB, C, D = [np.random.randn(*shapes[i]) for i in constants]\n\n# supplied ops are now mix of shapes and arrays\nops = (9, 5), B, C, D, (5, 8)\n\nexpr = oe.contract_expression(eq, *ops, constants=constants)\nexpr\n#&gt; &lt;ContractExpression('ij,[jk,kl,lm],mn-&gt;ni', constants=[1, 2, 3])&gt;\n</code></pre> <p>The expression now only takes the remaining two arrays as arguments (the tensors with <code>'ij'</code> and <code>'mn'</code> indices), and will store as many reusable constant contractions as possible.</p> <p>.. code:: python</p> <pre><code>A1, E1 = np.random.rand(*shapes[0]), np.random.rand(*shapes[-1])\nout1 = expr(A1, E1)\nout1.shap\n#&gt; (8, 9)\n\nA2, E2 = np.random.rand(*shapes[0]), np.random.rand(*shapes[-1])\nout2 = expr(A2, E2)\nout2.shape\n#&gt; (8, 9)\n\nnp.allclose(out1, out2)\n#&gt; False\n\nprint(expr)\n#&gt; &lt;ContractExpression('ij,[jk,kl,lm],mn-&gt;ni', constants=[1, 2, 3])&gt;\n#&gt;   1.  'jm,mn-&gt;jn' [GEMM]\n#&gt;   2.  'jn,ij-&gt;ni' [GEMM]\n</code></pre> <p>Where we can see that the expression now only has to perform two contractions to compute the output.</p> <p>Note</p> <p>The constant part of an expression is lazily generated upon the first call (specific to each backend), though it can also be explicitly built by calling <code>opt_einsum.contract.ContractExpression.evaluate_constants</code>.</p> <p>We can confirm the advantage of using expressions and constants by timing the following scenarios, first setting <code>A = np.random.rand(*shapes[0])</code> and <code>E = np.random.rand(*shapes[-1])</code>.</p>"},{"location":"getting_started/reusing_paths/#contract-from-scratch","title":"Contract from scratch","text":"<pre><code>%timeit oe.contract(eq, A, B, C, D, E)\n#&gt; 239 \u00b5s \u00b1 5.06 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>"},{"location":"getting_started/reusing_paths/#contraction-with-an-expression-but-no-constants","title":"Contraction with an expression but no constants","text":"<pre><code>expr_no_consts = oe.contract_expression(eq, *shapes)\n%timeit expr_no_consts(A, B, C, D, E)\n#&gt; 76.7 \u00b5s \u00b1 2.47 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre>"},{"location":"getting_started/reusing_paths/#contraction-with-an-expression-and-constants-marked","title":"Contraction with an expression and constants marked","text":"<pre><code>%timeit expr(A, E)\n#&gt; 40.8 \u00b5s \u00b1 1.22 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre> <p>Although this gives us a rough idea, of course the efficiency savings are hugely dependent on the size of the contraction and number of possible constant contractions.</p> <p>We also note that even if there are no constant contractions to perform, it can be very advantageous to specify constant tensors for particular backends. For instance, if a GPU backend is used, the constant tensors will be kept on the device rather than being transferred each time.</p>"},{"location":"getting_started/sharing_intermediates/","title":"Sharing Intermediates","text":"<p>If you want to compute multiple similar contractions with common terms, you can embed them in a <code>opt_einsum.shared_intermediates</code> context. Computations of subexpressions in this context will be memoized, and will be garbage collected when the contexts exits.</p> <p>For example, suppose we want to compute marginals at each point in a factor chain:</p> <pre><code>inputs = 'ab,bc,cd,de,ef'\nfactors = [np.random.rand(1000, 1000) for _ in range(5)]\n\n%%timeit\nmarginals = {output: contract('{}-&gt;{}'.format(inputs, output), *factors)\n             for output in 'abcdef'}\n#&gt; 1 loop, best of 3: 5.82 s per loop\n</code></pre> <p>To share this computation, we can perform all contractions in a shared context:</p> <pre><code>%%timeit\nwith shared_intermediates():\n    marginals = {output: contract('{}-&gt;{}'.format(inputs, output), *factors)\n                 for output in 'abcdef'}\n#&gt; 1 loop, best of 3: 1.55 s per loop\n</code></pre> <p>If it is difficult to fit your code into a context, you can instead save the sharing cache for later reuse.</p> <pre><code>with shared_intermediates() as cache:  # create a cache\n    pass\nmarginals = {}\nfor output in 'abcdef':\n    with shared_intermediates(cache):  # reuse a common cache\n        marginals[output] = contract('{}-&gt;{}'.format(inputs, output), *factors)\ndel cache  # garbage collect intermediates\n</code></pre> <p>Note that sharing contexts can be nested, so it is safe to to use <code>opt_einsum.shared_intermediates</code> in library code without leaking intermediates into user caches.</p> <p>Note</p> <p>By default a cache is thread safe, to share intermediates between threads explicitly pass the same cache to each thread.</p>"},{"location":"paths/branching_path/","title":"The Branching Path","text":"<p>While the <code>optimal</code> path is guaranteed to find the smallest estimate FLOP cost, it spends a lot of time exploring paths which are not likely to result in an optimal path. For instance, outer products are usually not advantageous unless absolutely necessary. Additionally, by trying a 'good' path first, it should be possible to quickly establish a threshold FLOP cost which can then be used to prune many bad paths.</p> <p>The branching strategy (provided by <code>opt_einsum.paths.branch</code>) does this by taking the recursive, depth-first approach of <code>opt_einsum.paths.optimal</code>, whilst also sorting potential contractions based on a heuristic cost, as in <code>opt_einsum.paths.greedy</code>.</p> <p>There are two main flavours:</p> <ul> <li><code>optimize='branch-all'</code>: explore all inner products, starting with   those that look best according to the cost heuristic.</li> <li><code>optimize='branch-2'</code>: similar, but at each step only explore the   estimated best two possible contractions, leading to a maximum of   2^N paths assessed.</li> </ul> <p>In both cases, <code>opt_einsum.paths.branch</code> takes an active approach to pruning paths well before they hit the best total FLOP count, by comparing them to the FLOP count (times some factor) achieved by the best path at the same point in the contraction.</p> <p>There is also <code>'branch-1'</code>, which, since it only explores a single path at each step does not really 'branch' - this is essentially the approach of <code>'greedy'</code>. In comparison, <code>'branch-1'</code> will be slower for large expressions, but for small to medium expressions it might find slightly higher quality contractions due to considering individual flop costs at each step.</p> <p>The default <code>optimize='auto'</code> mode of <code>opt_einsum</code> will use <code>'branch-all'</code> for 5 or 6 tensors, though it should be able to handle 12-13 tensors in a matter or seconds. Likewise, <code>'branch-2'</code> will be used for 7 or 8 tensors, though it should be able to handle 20-22 tensors in a matter of seconds. Finally, <code>'branch-1'</code> will be used by <code>'auto'</code> for expressions of up to 14 tensors.</p>"},{"location":"paths/branching_path/#customizing-the-branching-path","title":"Customizing the Branching Path","text":"<p>The 'branch and bound' path can be customized by creating a custom <code>opt_einsum.paths.BranchBound</code> instance. For example:</p> <pre><code>optimizer = oe.BranchBound(nbranch=3, minimize='size', cutoff_flops_factor=None)\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n</code></pre> <p>You could then tweak the settings (e.g. <code>optimizer.nbranch = 4</code>) and the best bound found so far will persist and be used to prune paths on the next call:</p> <pre><code>optimizer.nbranch = 4\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n</code></pre>"},{"location":"paths/custom_paths/","title":"Custom Path Optimizers","text":"<p>If you want to implement or just experiment with custom contaction paths then you can easily by subclassing the <code>opt_einsum.paths.PathOptimizer</code> object. For example, imagine we want to test the path that just blindly contracts the first pair of tensors again and again. We would implement this as:</p> <pre><code>import opt_einsum as oe\n\nclass MyOptimizer(oe.paths.PathOptimizer):\n\n    def __call__(self, inputs, output, size_dict, memory_limit=None):\n        return [(0, 1)] * (len(inputs) - 1)\n</code></pre> <p>Once defined we can use this as:</p> <pre><code>import numpy as np\n\n# set-up a random contraction\neq, shapes = oe.helpers.rand_equation(10, 3, seed=42)\narrays = list(map(np.ones, shapes))\n\n# set-up our optimizer and use it\noptimizer = MyOptimizer()\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n\nprint(path)\n#&gt; [(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\n\nprint(path_info.speedup)\n#&gt; 133.21363671496357\n</code></pre> <p>Note that though we still get a considerable speedup over <code>einsum</code> this is of course not a good strategy to take in general.</p>"},{"location":"paths/custom_paths/#custom-random-optimizers","title":"Custom Random Optimizers","text":"<p>If your custom path optimizer is inherently random, then you can reuse all the machinery of the random-greedy approach. Namely:</p> <ul> <li>A max-repeats or max-time approach</li> <li>Minimization with respect to total flops or largest intermediate size</li> <li>Parallelization using a pool-executor</li> </ul> <p>This is done by subclassing the <code>opt_einsum.paths.RandomOptimizer</code>  object and implementing a <code>setup</code> method. Here's an example where we just randomly select any path (again, although we get a considerable speedup over <code>einsum</code> this is not a good strategy to take in general):</p> <pre><code>from opt_einsum.path_random import ssa_path_compute_cost\n\nclass MyRandomOptimizer(oe.path_random.RandomOptimizer):\n\n    @staticmethod\n    def random_path(r, n, inputs, output, size_dict):\n        \"\"\"Picks a completely random contraction order.\n        \"\"\"\n        np.random.seed(r)\n        ssa_path = []\n        remaining = set(range(n))\n        while len(remaining) &gt; 1:\n            i, j = np.random.choice(list(remaining), size=2, replace=False)\n            remaining.add(n + len(ssa_path))\n            remaining.remove(i)\n            remaining.remove(j)\n            ssa_path.append((i, j))\n        cost, size = ssa_path_compute_cost(ssa_path, inputs, output, size_dict)\n        return ssa_path, cost, size\n\n    def setup(self, inputs, output, size_dict):\n        \"\"\"Prepares the function and arguments to repeatedly call.\n        \"\"\"\n        n = len(inputs)\n        trial_fn = self.random_path\n        trial_args = (n, inputs, output, size_dict)\n        return trial_fn, trial_args\n</code></pre> <p>Which we can now instantiate using various other options:</p> <pre><code>optimizer = MyRandomOptimizer(max_repeats=1000, max_time=10,\n                              parallel=True, minimize='size')\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n\nprint(path)\n#&gt; [(3, 4), (1, 3), (0, 3), (3, 5), (3, 4), (3, 4), (1, 0), (0, 1), (0, 1)]\n\nprint(path_info.speedup)\n#&gt; 712829.9451056132\n</code></pre> <p>There are a few things to note here:</p> <ol> <li>The core function (<code>MyRandomOptimizer.random_path</code> here), should take a    trial number <code>r</code> as it first argument</li> <li>It should return a ssa_path (see <code>opt_einsum.paths.ssa_to_linear</code> and    <code>opt_einsum.paths.linear_to_ssa</code>) as well as a flops-cost and max-size.</li> <li>The <code>setup</code> method prepares this function, as well as any input to it,    so that the trials will look roughly like    <code>[trial_fn(r, *trial_args) for r in range(max_repeats)]</code>. If you need to    parse the standard arguments (into a network for example), it thus only    needs to be done once per optimization</li> </ol> <p>More details about <code>opt_einsum.paths.RandomOptimizer</code> options can be found in <code>RandomGreedyPathPage</code> section.</p>"},{"location":"paths/dp_path/","title":"The Dynamic Programming Path","text":"<p>The dynamic programming (DP) approach described in reference [1] provides an efficient way to find an asymptotically optimal contraction path by running the following steps:</p> <ol> <li>Compute all traces, i.e. summations over indices occurring exactly in one    input.</li> <li>Decompose the contraction graph of inputs into disconnected subgraphs. Two    inputs are connected if they share at least one summation index.</li> <li>Find the contraction path for each of the disconnected subgraphs using a    DP approach: The optimal contraction path for all sets of <code>n</code> (ranging    from 1 to the number of inputs) connected tensors is found by combining    sets of <code>m</code> and <code>n-m</code> tensors.</li> </ol> <p>Note that computing all the traces in the very beginning can never lead to a non-optimal contraction path.</p> <p>Contractions of disconnected subgraphs can be optimized independently, which still results in an optimal contraction path. However, the computational complexity of finding the contraction path is drastically reduced: If the subgraphs consist of <code>n1</code>, <code>n2</code>, ... inputs, the computational complexity is reduced from <code>O(exp(n1 + n2 + ...))</code> to <code>O(exp(n1) + exp(n2) + ...)</code>.</p> <p>The DP approach will only perform pair contractions and by default will never compute intermediate outer products as in reference [1] it is shown that this always results in an asymptotically optimal contraction path.</p> <p>A major optimization for DP is the cost capping strategy: The DP optimization only memorizes contractions for a subset of inputs, if the total cost for this contraction is smaller than the cost cap. The cost cap is initialized with the minimal possible cost, i.e. the product of all output dimensions, and is iteratively increased by multiplying it with the smallest dimension until a contraction path including all inputs is found.</p> <p>Note that the worst case scaling of DP is exponential in the number of inputs. Nevertheless, if the contraction graph is not completely random, but exhibits a certain kind of structure, it can be used for large contraction graphs and is guaranteed to find an asymptotically optimal contraction path. For this reason it is the most frequently used contraction path optimizer in the field of tensor network states.</p> <p>More specifically, the search is performed over connected subgraphs, which, for example, planar and tree-like graphs have far fewer of. As a rough guide, if the graph is planar, expressions with many tens of tensors are tractable, whereas if the graph is tree-like, expressions with many hundreds of tensors are tractable.</p> <p>[1] Robert N. C. Pfeifer, Jutho Haegeman, and Frank Verstraete Phys. Rev. E 90, 033315 (2014). https://arxiv.org/abs/1304.6112</p>"},{"location":"paths/dp_path/#customizing-the-dynamic-programming-path","title":"Customizing the Dynamic Programming Path","text":"<p>The default <code>optimize='dp'</code> approach has sensible defaults but can be customized with the <code>opt_einsum.paths.DynamicProgramming</code> object.</p> <pre><code>import opt_einsum as oe\n\noptimizer = oe.DynamicProgramming(\n    minimize='size',    # optimize for largest intermediate tensor size\n    search_outer=True,  # search through outer products as well\n    cost_cap=False,     # don't use cost-capping strategy\n)\n\noe.contract(eq, *arrays, optimize=optimizer)\n</code></pre> <p>Warning</p> <p>Note that searching outer products will most likely drastically slow down the optimizer on all but the smallest examples.</p> <p>The values that <code>minimize</code> can take are:</p> <ul> <li><code>'flops'</code>: minimize the total number of scalar operations.</li> <li><code>'size'</code>: minimize the size of the largest intermediate.</li> <li><code>'write'</code>: minimize the combined size of all intermediate tensors -    approximately speaking the amount of memory that will be written. This is    relevant if you were to automatically differentiate through the    contraction, which naively would require storing all intermediates.</li> <li><code>'combo'</code> - minimize <code>flops + alpha * write</code> summed over intermediates, a   default ratio of <code>alpha=64</code> is used, or it can be customized with   <code>f'combo-{alpha}'</code>.</li> <li><code>'limit'</code> - minimize <code>max(flops, alpha * write)</code> summed over intermediates, a   default ratio of <code>alpha=64</code> is used, or it can be customized with <code>f'limit-{alpha}'</code>.</li> </ul> <p>The last two take into account the fact that real contraction performance can be bound by memory speed, and so favor paths with higher arithmetic intensity. The default value of <code>alpha=64</code> is reasonable for both typical CPUs and GPUs.</p>"},{"location":"paths/greedy_path/","title":"The Greedy Path","text":"<p>The <code>'greedy'</code> approach provides a very efficient strategy for finding contraction paths for expressions with large numbers of tensors. It does this by eagerly choosing contractions in three stages:</p> <ol> <li>Eagerly compute any Hadamard products (in arbitrary order -- this is    commutative).</li> <li>Greedily contract pairs of remaining tensors, at each step choosing the    pair that maximizes <code>reduced_size</code> -- these are generally inner    products.</li> <li>Greedily compute any pairwise outer products, at each step choosing    the pair that minimizes <code>sum(input_sizes)</code>.</li> </ol> <p>The cost heuristic <code>reduced_size</code> is simply the size of the pair of potential tensors to be contracted, minus the size of the resulting tensor.</p> <p>The <code>greedy</code> algorithm has space and time complexity <code>O(n * k)</code> where <code>n</code> is the number of input tensors and <code>k</code> is the maximum number of tensors that share any dimension (excluding dimensions that occur in the output or in every tensor). As such, the algorithm scales well to very large sparse contractions of low-rank tensors, and indeed, often finds the optimal, or close to optimal path in such cases.</p> <p>The <code>greedy</code> functionality is provided by <code>opt_einsum.paths.greedy</code>, and is selected by the default <code>optimize='auto'</code> mode of <code>opt_einsum</code> for expressions with many inputs. Expressions of up to a thousand tensors should still take well less than a second to find paths for.</p>"},{"location":"paths/greedy_path/#optimal-scaling-misses","title":"Optimal Scaling Misses","text":"<p>The greedy algorithm, while inexpensive, can occasionally miss optimal scaling in some circumstances as seen below. The <code>greedy</code> algorithm prioritizes expressions which remove the largest indices first, in this particular case this is the incorrect choice and it is difficult for any heuristic algorithm to \"see ahead\" as would be needed here.</p> <p>It should be stressed these cases are quite rare and by default <code>contract</code> uses the <code>optimal</code> path for four and fewer inputs as the cost of evaluating the <code>optimal</code> path is similar to that of the <code>greedy</code> path. Similarly, for 5-8 inputs, <code>contract</code> uses one of the branching strategies which can find higher quality paths.</p> <pre><code>M = np.random.rand(35, 37, 59)\nA = np.random.rand(35, 51, 59)\nB = np.random.rand(37, 51, 51, 59)\nC = np.random.rand(59, 27)\n\npath, desc = oe.contract_path('xyf,xtf,ytpf,fr-&gt;tpr', M, A, B, C, optimize=\"greedy\")\nprint(desc)\n#&gt;   Complete contraction:  xyf,xtf,ytpf,fr-&gt;tpr\n#&gt;          Naive scaling:  6\n#&gt;      Optimized scaling:  5\n#&gt;       Naive FLOP count:  2.146e+10\n#&gt;   Optimized FLOP count:  4.165e+08\n#&gt;    Theoretical speedup:  51.533\n#&gt;   Largest intermediate:  5.371e+06 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    5          False         ytpf,xyf-&gt;tpfx                      xtf,fr,tpfx-&gt;tpr\n#&gt;    4          False          tpfx,xtf-&gt;tpf                           fr,tpf-&gt;tpr\n#&gt;    4           GEMM            tpf,fr-&gt;tpr                              tpr-&gt;tpr\n\npath, desc = oe.contract_path('xyf,xtf,ytpf,fr-&gt;tpr', M, A, B, C, optimize=\"optimal\")\nprint(desc)\n#&gt;   Complete contraction:  xyf,xtf,ytpf,fr-&gt;tpr\n#&gt;          Naive scaling:  6\n#&gt;      Optimized scaling:  4\n#&gt;       Naive FLOP count:  2.146e+10\n#&gt;   Optimized FLOP count:  2.744e+07\n#&gt;    Theoretical speedup:  782.283\n#&gt;   Largest intermediate:  1.535e+05 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    4          False           xtf,xyf-&gt;tfy                      ytpf,fr,tfy-&gt;tpr\n#&gt;    4          False          tfy,ytpf-&gt;tfp                           fr,tfp-&gt;tpr\n#&gt;    4           TDOT            tfp,fr-&gt;tpr                              tpr-&gt;tpr\n</code></pre> <p>So we can see that the <code>greedy</code> algorithm finds a path which is about 16 times slower than the <code>optimal</code> one. In such cases, it might be worth using one of the more exhaustive optimization strategies: <code>'optimal'</code>, <code>'branch-all'</code> or <code>branch-2</code> (all of which will find the optimal path in this example).</p>"},{"location":"paths/greedy_path/#customizing-the-greedy-path","title":"Customizing the Greedy Path","text":"<p>The greedy path is a local optimizer in that it only ever assesses pairs of tensors to contract, assigning each a heuristic 'cost' and then choosing the 'best' of these. Custom greedy approaches can be implemented by supplying callables to the <code>cost_fn</code> and <code>choose_fn</code> arguments of <code>opt_einsum.paths.greedy</code>.</p>"},{"location":"paths/introduction/","title":"Introduction","text":"<p>Performing an optimized tensor contraction to speed up <code>einsum</code> involves two key stages:</p> <ol> <li>Finding a pairwise contraction order, or 'path'.</li> <li>Performing the sequence of contractions given this path.</li> </ol> <p>The better the quality of path found in the first step, the quicker the actual contraction in the second step can be -- often dramatically. However, finding the optimal path is an NP-hard problem that can quickly become intractable, meaning that a  balance must be struck between the time spent finding a path, and its quality. <code>opt_einsum</code> handles this by using several path finding algorithms, which can be manually specified using the <code>optimize</code> keyword. These are:</p> <ul> <li>The <code>'optimal'</code> strategy - an exhaustive search of all possible paths</li> <li>The <code>'dynamic-programming'</code> strategy - a near-optimal search based off dynamic-programming</li> <li>The <code>'branch'</code> strategy - a more restricted search of many likely paths</li> <li>The <code>'greedy'</code> strategy - finds a path one step at a time using a cost   heuristic</li> </ul> <p>By default (<code>optimize='auto'</code>), <code>opt_einsum.contract</code> will select the best of these it can while aiming to keep path finding times below around 1ms. An analysis of each of these approaches' performance can be found at the bottom of this page.</p> <p>For large and complex contractions, there is the <code>'random-greedy'</code> approach, which samples many (by default 32) greedy paths and can be customized to explicitly spend a maximum amount of time searching. Another preset, <code>'random-greedy-128'</code>, uses 128 paths for a more exhaustive search. See <code>RandomGreedyPath</code> page for more details on configuring these.</p> <p>Finally, there is the <code>'auto-hq'</code> preset which targets a much larger search time (~1sec) in return for finding very high quality paths, dispatching to the <code>'optimal'</code>, <code>'dynamic-programming'</code> and then <code>'random-greedy-128'</code> paths depending on contraction size.</p> <p>If you want to find the path separately to performing the contraction, or just inspect information about the path found, you can use the function <code>opt_einsum.contract_path</code>.</p>"},{"location":"paths/introduction/#examining-the-path","title":"Examining the Path","text":"<p>As an example, consider the following expression found in a perturbation theory (one of ~5,000 such expressions):</p> <pre><code>'bdik,acaj,ikab,ajac,ikbd'\n</code></pre> <p>At first, it would appear that this scales like N^7 as there are 7 unique indices; however, we can define a intermediate to reduce this scaling.</p> <pre><code># (N^5 scaling)\na = 'bdik,ikab,ikbd'\n\n# (N^4 scaling)\nresult = 'acaj,ajac,a'\n</code></pre> <p>This is a single possible path to the final answer (and notably, not the most optimal) out of many possible paths. Now, let opt_einsum compute the optimal path:</p> <pre><code>import opt_einsum as oe\n\n# Take a complex string\neinsum_string = 'bdik,acaj,ikab,ajac,ikbd-&gt;'\n\n# Build random views to represent this contraction\nunique_inds = set(einsum_string) - {',', '-', '&gt;'}\nindex_size = [10, 17, 9, 10, 13, 16, 15, 14, 12]\nsizes_dict = dict(zip(unique_inds, index_size))\nviews = oe.helpers.build_views(einsum_string, sizes_dict)\n\npath, path_info = oe.contract_path(einsum_string, *views)\n\nprint(path)\n#&gt; [(0, 4), (1, 3), (0, 1), (0, 1)]\n\nprint(path_info)\n#&gt;   Complete contraction:  bdik,acaj,ikab,ajac,ikbd-&gt;\n#&gt;          Naive scaling:  7\n#&gt;      Optimized scaling:  4\n#&gt;       Naive FLOP count:  2.387e+8\n#&gt;   Optimized FLOP count:  8.068e+4\n#&gt;    Theoretical speedup:  2958.354\n#&gt;   Largest intermediate:  1.530e+3 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    4              0         ikbd,bdik-&gt;ikb                  acaj,ikab,ajac,ikb-&gt;\n#&gt;    4    GEMV/EINSUM            ikb,ikab-&gt;a                         acaj,ajac,a-&gt;\n#&gt;    3              0           ajac,acaj-&gt;a                                 a,a-&gt;\n#&gt;    1            DOT                  a,a-&gt;                                    -&gt;\n</code></pre> <p>We can then check that actually performing the contraction produces the expected result:</p> <pre><code>import numpy as np\n\neinsum_result = np.einsum(\"bdik,acaj,ikab,ajac,ikbd-&gt;\", *views)\ncontract_result = oe.contract(\"bdik,acaj,ikab,ajac,ikbd-&gt;\", *views)\n\nnp.allclose(einsum_result, contract_result)\n#&gt; True\n</code></pre> <p>By contracting terms in the correct order we can see that this expression can be computed with N^4 scaling. Even with the overhead of finding the best order or 'path' and small dimensions, <code>opt_einsum</code> is roughly 3000 times faster than pure einsum for this expression.</p>"},{"location":"paths/introduction/#format-of-the-path","title":"Format of the Path","text":"<p>Let us look at the structure of a canonical <code>einsum</code> path found in NumPy and its optimized variant:</p> <pre><code>einsum_path = [(0, 1, 2, 3, 4)]\nopt_path = [(1, 3), (0, 2), (0, 2), (0, 1)]\n</code></pre> <p>In opt_einsum each element of the list represents a single contraction. In the above example the einsum_path would effectively compute the result as a single contraction identical to that of <code>einsum</code>, while the opt_path would perform four contractions in order to reduce the overall scaling. The first tuple in the opt_path, <code>(1,3)</code>, pops the second and fourth terms, then contracts them together to produce a new term which is then appended to the list of terms, this is continued until all terms are contracted. An example should illuminate this:</p> <pre><code>---------------------------------------------------------------------------------\nscaling   GEMM                   current                                remaining\n---------------------------------------------------------------------------------\nterms = ['bdik', 'acaj', 'ikab', 'ajac', 'ikbd'] contraction = (1, 3)\n  3     False              ajac,acaj-&gt;a                       bdik,ikab,ikbd,a-&gt;\nterms = ['bdik', 'ikab', 'ikbd', 'a'] contraction = (0, 2)\n  4     False            ikbd,bdik-&gt;bik                             ikab,a,bik-&gt;\nterms = ['ikab', 'a', 'bik'] contraction = (0, 2)\n  4     False              bik,ikab-&gt;a                                    a,a-&gt;\nterms = ['a', 'a'] contraction = (0, 1)\n  1       DOT                    a,a-&gt;                                       -&gt;\n</code></pre> <p>A path specified in this format can explicitly be supplied directly to <code>opt_einsum.contract</code> using the <code>optimize</code> keyword:</p> <pre><code>contract_result = oe.contract(\"bdik,acaj,ikab,ajac,ikbd-&gt;\", *views, optimize=opt_path)\n\nnp.allclose(einsum_result, contract_result)\n#&gt; True\n</code></pre>"},{"location":"paths/introduction/#performance-comparison","title":"Performance Comparison","text":"<p>The following graphs should give some indication of the tradeoffs between path finding time and path quality. They are generated by finding paths with each possible algorithm for many randomly generated networks of <code>n</code> tensors with varying connectivity.</p> <p>First we have the time to find each path as a function of the number of terms in the expression:</p> <p></p> <p>Clearly the exhaustive (<code>'optimal'</code>, <code>'branch-all'</code>) and exponential (<code>'branch-2'</code>) searches eventually scale badly, but for modest amounts of terms they incur only a small overhead. The <code>'random-greedy'</code> approach is not shown here as it is simply <code>max_repeats</code> times slower than the <code>'greedy'</code> approach - at least if not parallelized.</p> <p>Next we can look at the average FLOP speedup (as compared to the easiest path to find, <code>'greedy'</code>):</p> <p></p> <p>One can see that the hierarchy of path qualities is:</p> <ol> <li><code>'optimal'</code> (used by auto for <code>n &lt;= 4</code>)</li> <li><code>'branch-all'</code> (used by auto for <code>n &lt;= 6</code>)</li> <li><code>'branch-2'</code> (used by auto for <code>n &lt;= 8</code>)</li> <li><code>'branch-1'</code> (used by auto for <code>n &lt;= 14</code>)</li> <li><code>'greedy'</code> (used by auto for anything larger)</li> </ol> <p>Note</p> <p>The performance of the <code>'random=greedy'</code> approach (which is never used automatically) can be found separately in <code>RandomGreedyPath</code> section.</p> <p>There are a few important caveats to note with this graph. Firstly, the benefits of more advanced path finding are very dependent on the complexity of the expression. For 'simple' contractions, all the different approaches will mostly find the same path (as here). However, for 'tricky' contractions, there will be certain cases where the more advanced algorithms will find much better paths. As such, while this graph gives a good idea of the relative performance of each algorithm, the 'average speedup' is not a perfect indicator since worst-case performance might be more critical.</p> <p>Note that the speedups for any of the methods as compared to a standard <code>einsum</code> or a naively chosen path (such as <code>path=[(0, 1), (0, 1), ...]</code>) are all exponentially large and not shown.</p>"},{"location":"paths/optimal_path/","title":"The Optimal Path","text":"<p>The most optimal path can be found by searching through every possible way to contract the tensors together, this includes all combinations with the new intermediate tensors as well. While this algorithm scales like N!, and can often become more costly to compute than the unoptimized contraction itself, it provides an excellent benchmark. The function that computes this path in opt_einsum is called <code>opt_einsum.paths.optimal</code> and works by performing a recursive, depth-first search. By keeping track of the best path found so far, in terms of total estimated FLOP count, the search can then quickly prune many paths as soon as as they exceed this best. This optimal strategy is used by default with the <code>optimize='auto'</code> mode of <code>opt_einsum</code> for 4 tensors or less, though it can handle expressions of up to 9-10 tensors in a matter of seconds.</p> <p>Let us look at an example:</p> <pre><code>Contraction:  abc,dc,ac-&gt;bd\n</code></pre> <p>Build a list with tuples that have the following form:</p> <pre><code>#&gt; iteration 0:\n#&gt;  \"(cost, path,  list of input sets remaining)\"\n#&gt; [ (0,    [],    [set(['a', 'c', 'b']), set(['d', 'c']), set(['a', 'c'])] ]\n</code></pre> <p>Since this is iteration zero, we have the initial list of input sets. We can consider three possible combinations where we contract list positions (0, 1), (0, 2), or (1, 2) together:</p> <pre><code>#&gt; iteration 1:\n#&gt; [ (9504, [(0, 1)], [set(['a', 'c']), set(['a', 'c', 'b', 'd'])  ]),\n#&gt;   (1584, [(0, 2)], [set(['c', 'd']), set(['c', 'b'])            ]),\n#&gt;   (864,  [(1, 2)], [set(['a', 'c', 'b']), set(['a', 'c', 'd'])  ])]\n</code></pre> <p>We have now run through the three possible combinations, computed the cost of the contraction up to this point, and appended the resulting indices from the contraction to the list. As all contractions only have two remaining input sets the only possible contraction is (0, 1):</p> <pre><code>#&gt; iteration 2:\n#&gt; [ (28512, [(0, 1), (0, 1)], [set(['b', 'd'])  ]),\n#&gt;   (3168,  [(0, 2), (0, 1)], [set(['b', 'd'])  ]),\n#&gt;   (19872, [(1, 2), (0, 1)], [set(['b', 'd'])  ])]\n</code></pre> <p>The final contraction cost is computed, and we choose the second path from the list as the overall cost is the lowest.</p>"},{"location":"paths/random_greedy_path/","title":"The Random-Greedy Path","text":"<p>For large and complex contractions the exhaustive approaches will be too slow while the greedy path might be very far from optimal. In this case you might want to consider the <code>'random-greedy'</code> path optimizer. This samples many greedy paths and selects the best one found, which can often be exponentially better than the average.</p> <pre><code>import opt_einsum as oe\nimport numpy as np\nimport math\n\neq, shapes = oe.helpers.rand_equation(40, 5, seed=1, d_max=2)\narrays = list(map(np.ones, shapes))\n\npath_greedy = oe.contract_path(eq, *arrays, optimize='greedy')[1]\nprint(math.log2(path_greedy.opt_cost))\n#&gt; 36.04683022558587\n\npath_rand_greedy = oe.contract_path(eq, *arrays, optimize='random-greedy')[1]\nprint(math.log2(path_rand_greedy.opt_cost))\n#&gt; 32.203616699170865\n</code></pre> <p>So here the random-greedy approach has found a path about 16 times quicker (<code>= 2^(36 - 32)</code>).</p> <p>This approach works by randomly choosing from the best <code>n</code> contractions at each step, weighted by a Boltzmann factor with respect to the contraction with the 'best' cost. As such, contractions with very similar costs will be explored with equal probability, whereas those with higher costs will be less likely, but still possible. In this way, the optimizer can randomly explore the huge space of possible paths, but in a guided manner.</p> <p>The following graph roughly demonstrates the potential benefits of the <code>'random-greedy'</code> algorithm, here for large randomly generated contractions, with either 8, 32 (the default), or 128 repeats:</p> <p></p> <p>Note</p> <p>Bear in mind that such speed-ups are not guaranteed - it very much depends on how structured or complex your contractions are.</p>"},{"location":"paths/random_greedy_path/#customizing-the-random-greedy-path","title":"Customizing the Random-Greedy Path","text":"<p>The random-greedy optimizer can be customized by instantiating your own <code>opt_einsum.paths.RandomGreedy</code> object. Here you can control:</p> <ul> <li><code>temperature</code> - how far to stray from the locally 'best' contractions</li> <li><code>rel_temperature</code> - whether to normalize the temperature</li> <li><code>nbranch</code> - how many contractions (branches) to consider at each step</li> <li><code>cost_fn</code> - how to cost potential contractions</li> </ul> <p>There are also the main <code>opt_einsum.paths.RandomOptimizer</code> options:</p> <ul> <li><code>max_repeats</code> - the maximum number of repeats</li> <li><code>max_time</code> - the maximum amount of time to run for (in seconds)</li> <li><code>minimize</code> - whether to minimize for total <code>'flops'</code> or <code>'size'</code> of the   largest intermediate</li> </ul> <p>For example, here we'll create an optimizer, then change its temperature whilst reusing it. We'll also set a high <code>max_repeats</code> and instead use a maximum time to terminate the search:</p> <pre><code>optimizer = oe.RandomGreedy(max_time=2, max_repeats=1_000_000)\n\nfor T in [1000, 100, 10, 1, 0.1]:\n    optimizer.temperature = T\n    path_rand_greedy = oe.contract_path(eq, *arrays, optimize=optimizer)[1]\n    print(math.log2(optimizer.best['flops']))\n\n#&gt; 32.81709395639357\n#&gt; 32.67625007170783\n#&gt; 31.719756871539033\n#&gt; 31.62043317835677\n#&gt; 31.253305891247\n\n# the total number of trials so far\nprint(len(optimizer.costs))\n#&gt; 2555\n</code></pre> <p>So we have improved a bit on the standard <code>'random-greedy'</code> (which performs 32 repeats by default). The <code>optimizer</code> object now stores both the best path found so far - <code>optimizer.path</code> - as well as the list of flop-costs and maximum sizes found for each trial - <code>optimizer.costs</code> and <code>optimizer.sizes</code> respectively.</p>"},{"location":"paths/random_greedy_path/#parallelizing-the-random-greedy-search","title":"Parallelizing the Random-Greedy Search","text":"<p>Since each greedy attempt is independent, the random-greedy approach is naturally suited to parallelization. This can be automatically handled by specifying the <code>parallel</code> keyword like so:</p> <pre><code># use same number of processes as cores\noptimizer = oe.RandomGreedy(parallel=True)\n\n# or use specific number of processes\noptimizer = oe.RandomGreedy(parallel=4)\n</code></pre> <p>Warning</p> <p>The pool-executor used to perform this parallelization is the <code>ProcessPoolExecutor</code> from the <code>concurrent.futures</code>  module.</p> <p>For full control over the parallelization you can supply any pool-executor like object, which should have an API matching the Python 3 concurrent.futures module:</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\n\npool = ProcessPoolExecutor()\noptimizer = oe.RandomGreedy(parallel=pool, max_repeats=128)\npath_rand_greedy = oe.contract_path(eq, *arrays, optimize=optimizer)[1]\n\nprint(math.log2(optimizer.best['flops']))\n#&gt; 31.64992600300931\n</code></pre> <p>Other examples of such pools include:</p> <ul> <li>loky</li> <li>dask.distributed</li> <li>mpi4py</li> </ul>"}]}